{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_excel(\"../../data/unzipped/Train.xlsx\", skiprows=range(1,2), index_col=0, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Диффузный индекс цен на выпускаемую продукцию, ожидаемые изменения</th>\n",
       "      <th>Диффузный индекс цен на покупаемую продукцию, ожидаемые изменения</th>\n",
       "      <th>Диффузный индекс заработной платы, ожидаемые изменения</th>\n",
       "      <th>Диффузный индекс занятости, ожидаемые изменения</th>\n",
       "      <th>Диффузный индекс выпуска, ожидаемые изменения</th>\n",
       "      <th>Диффузный индекс закупок оборудования, ожидаемые изменения</th>\n",
       "      <th>Диффузный индекс финансового состояния, ожидаемые изменения</th>\n",
       "      <th>Диффузный индекс портфеля заказов, ожидаемые изменения</th>\n",
       "      <th>Диффузный индекс задолженности банкам, ожидаемые изменения</th>\n",
       "      <th>Производство скота и птицы, тыс.тонн</th>\n",
       "      <th>...</th>\n",
       "      <th>Среднемесячная пенсия, руб.</th>\n",
       "      <th>Реальная пенсия</th>\n",
       "      <th>MIACR</th>\n",
       "      <th>Ставка по краткосрочным кредитам, население</th>\n",
       "      <th>Ставка по долгосрочным кредитам, население</th>\n",
       "      <th>Ставка по краткосрочным кредитам, фирмы</th>\n",
       "      <th>Ставка по долгосрочным кредитам, фирмы</th>\n",
       "      <th>Краткосрочные кредиты и прочие средства нефинансовым организациям, млрд.руб</th>\n",
       "      <th>Долгосрочные кредиты и прочие средства нефинансовым организациям, млрд.руб</th>\n",
       "      <th>Цена Urals, долл/бар</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003m01</th>\n",
       "      <td>74</td>\n",
       "      <td>93</td>\n",
       "      <td>61</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>28</td>\n",
       "      <td>47</td>\n",
       "      <td>44</td>\n",
       "      <td>22</td>\n",
       "      <td>6.269096</td>\n",
       "      <td>...</td>\n",
       "      <td>7.288791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.331</td>\n",
       "      <td>21.5</td>\n",
       "      <td>18.9</td>\n",
       "      <td>14.5</td>\n",
       "      <td>17.3</td>\n",
       "      <td>6.993658</td>\n",
       "      <td>6.290272</td>\n",
       "      <td>3.383413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003m02</th>\n",
       "      <td>78</td>\n",
       "      <td>95</td>\n",
       "      <td>66</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>...</td>\n",
       "      <td>7.313953</td>\n",
       "      <td>0.008960</td>\n",
       "      <td>2.514</td>\n",
       "      <td>22.2</td>\n",
       "      <td>17.8</td>\n",
       "      <td>14.1</td>\n",
       "      <td>19.6</td>\n",
       "      <td>7.019476</td>\n",
       "      <td>6.298398</td>\n",
       "      <td>3.420624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003m03</th>\n",
       "      <td>84</td>\n",
       "      <td>98</td>\n",
       "      <td>72</td>\n",
       "      <td>45</td>\n",
       "      <td>56</td>\n",
       "      <td>29</td>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>6.361302</td>\n",
       "      <td>...</td>\n",
       "      <td>7.314153</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>2.625</td>\n",
       "      <td>18.3</td>\n",
       "      <td>19.9</td>\n",
       "      <td>13.3</td>\n",
       "      <td>16.4</td>\n",
       "      <td>7.031299</td>\n",
       "      <td>6.325613</td>\n",
       "      <td>3.363693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003m04</th>\n",
       "      <td>83</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>40</td>\n",
       "      <td>66</td>\n",
       "      <td>38</td>\n",
       "      <td>59</td>\n",
       "      <td>69</td>\n",
       "      <td>23</td>\n",
       "      <td>6.269096</td>\n",
       "      <td>...</td>\n",
       "      <td>7.382809</td>\n",
       "      <td>0.057178</td>\n",
       "      <td>1.888</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.3</td>\n",
       "      <td>13.4</td>\n",
       "      <td>14.9</td>\n",
       "      <td>7.048821</td>\n",
       "      <td>6.343001</td>\n",
       "      <td>3.130242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003m05</th>\n",
       "      <td>79</td>\n",
       "      <td>94</td>\n",
       "      <td>76</td>\n",
       "      <td>43</td>\n",
       "      <td>66</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>68</td>\n",
       "      <td>23</td>\n",
       "      <td>6.177944</td>\n",
       "      <td>...</td>\n",
       "      <td>7.382809</td>\n",
       "      <td>0.049146</td>\n",
       "      <td>1.308</td>\n",
       "      <td>23.1</td>\n",
       "      <td>21.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.076400</td>\n",
       "      <td>6.371099</td>\n",
       "      <td>3.210488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Диффузный индекс цен на выпускаемую продукцию, ожидаемые изменения   \\\n",
       "2003m01                                                 74                     \n",
       "2003m02                                                 78                     \n",
       "2003m03                                                 84                     \n",
       "2003m04                                                 83                     \n",
       "2003m05                                                 79                     \n",
       "\n",
       "         Диффузный индекс цен на покупаемую продукцию, ожидаемые изменения   \\\n",
       "2003m01                                                 93                    \n",
       "2003m02                                                 95                    \n",
       "2003m03                                                 98                    \n",
       "2003m04                                                 95                    \n",
       "2003m05                                                 94                    \n",
       "\n",
       "         Диффузный индекс заработной платы, ожидаемые изменения   \\\n",
       "2003m01                                                 61         \n",
       "2003m02                                                 66         \n",
       "2003m03                                                 72         \n",
       "2003m04                                                 70         \n",
       "2003m05                                                 76         \n",
       "\n",
       "         Диффузный индекс занятости, ожидаемые изменения   \\\n",
       "2003m01                                                40   \n",
       "2003m02                                                37   \n",
       "2003m03                                                45   \n",
       "2003m04                                                40   \n",
       "2003m05                                                43   \n",
       "\n",
       "         Диффузный индекс выпуска, ожидаемые изменения   \\\n",
       "2003m01                                              44   \n",
       "2003m02                                              49   \n",
       "2003m03                                              56   \n",
       "2003m04                                              66   \n",
       "2003m05                                              66   \n",
       "\n",
       "         Диффузный индекс закупок оборудования, ожидаемые изменения   \\\n",
       "2003m01                                                 28             \n",
       "2003m02                                                 30             \n",
       "2003m03                                                 29             \n",
       "2003m04                                                 38             \n",
       "2003m05                                                 33             \n",
       "\n",
       "         Диффузный индекс финансового состояния, ожидаемые изменения   \\\n",
       "2003m01                                                 47              \n",
       "2003m02                                                 53              \n",
       "2003m03                                                 51              \n",
       "2003m04                                                 59              \n",
       "2003m05                                                 63              \n",
       "\n",
       "         Диффузный индекс портфеля заказов, ожидаемые изменения   \\\n",
       "2003m01                                                 44         \n",
       "2003m02                                                 56         \n",
       "2003m03                                                 61         \n",
       "2003m04                                                 69         \n",
       "2003m05                                                 68         \n",
       "\n",
       "         Диффузный индекс задолженности банкам, ожидаемые изменения   \\\n",
       "2003m01                                                 22             \n",
       "2003m02                                                 18             \n",
       "2003m03                                                 25             \n",
       "2003m04                                                 23             \n",
       "2003m05                                                 23             \n",
       "\n",
       "         Производство скота и птицы, тыс.тонн   ...  \\\n",
       "2003m01                               6.269096  ...   \n",
       "2003m02                               6.320768  ...   \n",
       "2003m03                               6.361302  ...   \n",
       "2003m04                               6.269096  ...   \n",
       "2003m05                               6.177944  ...   \n",
       "\n",
       "         Среднемесячная пенсия, руб.  Реальная пенсия  MIACR  \\\n",
       "2003m01                     7.288791         0.000000  6.331   \n",
       "2003m02                     7.313953         0.008960  2.514   \n",
       "2003m03                     7.314153        -0.001091  2.625   \n",
       "2003m04                     7.382809         0.057178  1.888   \n",
       "2003m05                     7.382809         0.049146  1.308   \n",
       "\n",
       "         Ставка по краткосрочным кредитам, население  \\\n",
       "2003m01                                         21.5   \n",
       "2003m02                                         22.2   \n",
       "2003m03                                         18.3   \n",
       "2003m04                                         20.0   \n",
       "2003m05                                         23.1   \n",
       "\n",
       "         Ставка по долгосрочным кредитам, население  \\\n",
       "2003m01                                        18.9   \n",
       "2003m02                                        17.8   \n",
       "2003m03                                        19.9   \n",
       "2003m04                                        21.3   \n",
       "2003m05                                        21.5   \n",
       "\n",
       "         Ставка по краткосрочным кредитам, фирмы  \\\n",
       "2003m01                                     14.5   \n",
       "2003m02                                     14.1   \n",
       "2003m03                                     13.3   \n",
       "2003m04                                     13.4   \n",
       "2003m05                                     12.5   \n",
       "\n",
       "         Ставка по долгосрочным кредитам, фирмы  \\\n",
       "2003m01                                    17.3   \n",
       "2003m02                                    19.6   \n",
       "2003m03                                    16.4   \n",
       "2003m04                                    14.9   \n",
       "2003m05                                    15.0   \n",
       "\n",
       "         Краткосрочные кредиты и прочие средства нефинансовым организациям, млрд.руб  \\\n",
       "2003m01                                           6.993658                             \n",
       "2003m02                                           7.019476                             \n",
       "2003m03                                           7.031299                             \n",
       "2003m04                                           7.048821                             \n",
       "2003m05                                           7.076400                             \n",
       "\n",
       "         Долгосрочные кредиты и прочие средства нефинансовым организациям, млрд.руб  \\\n",
       "2003m01                                           6.290272                            \n",
       "2003m02                                           6.298398                            \n",
       "2003m03                                           6.325613                            \n",
       "2003m04                                           6.343001                            \n",
       "2003m05                                           6.371099                            \n",
       "\n",
       "         Цена Urals, долл/бар  \n",
       "2003m01              3.383413  \n",
       "2003m02              3.420624  \n",
       "2003m03              3.363693  \n",
       "2003m04              3.130242  \n",
       "2003m05              3.210488  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def p_val(x):\n",
    "    return 2 * min(norm.cdf(-x), norm.cdf(x))\n",
    "\n",
    "def z_test(res1, res2):\n",
    "    res2 = np.array(res2)\n",
    "    res1 = np.array(res1)\n",
    "    delta = res1 - res2\n",
    "    mu_z = np.mean(delta)\n",
    "    sigma = np.std(delta)\n",
    "    z = mu_z / sigma * T ** 0.5\n",
    "    p = p_val(z)\n",
    "    win_rate = np.mean(res1 >= res2) \n",
    "    return {\"p_value\":p, \"mu\": mu_z, \"sigma\": sigma, \"win_rate\": win_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_prefix(X):\n",
    "    X = np.array(X, dtype=float)\n",
    "    last_nan = np.max(list(np.where(np.isnan(X))[0]) + [-1])\n",
    "    X = X[last_nan + 1:]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adfuller_vals = []\n",
    "for i in range(train_data.shape[1]):\n",
    "    X = train_data.values[:, i]\n",
    "    X = remove_nan_prefix(X)\n",
    "    decompose_result = seasonal_decompose(X, model=\"additive\", period=12)\n",
    "\n",
    "    trend = decompose_result.trend\n",
    "    seasonal = decompose_result.seasonal\n",
    "    residual = decompose_result.resid\n",
    "\n",
    "    diff = X - seasonal\n",
    "    diff = (diff - np.mean(diff)) / np.std(diff)\n",
    "    result = adfuller(diff)\n",
    "    adfuller_vals.append(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5., 2., 2., 4., 2., 2., 4., 3., 1., 0., 1., 1., 3., 3., 1., 3., 4.,\n",
       "        3., 1., 1., 2., 1., 0., 1., 2., 4., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 3.]),\n",
       " array([0.00127187, 0.02122641, 0.04118095, 0.06113549, 0.08109003,\n",
       "        0.10104457, 0.12099911, 0.14095365, 0.16090819, 0.18086273,\n",
       "        0.20081727, 0.22077181, 0.24072635, 0.26068089, 0.28063543,\n",
       "        0.30058997, 0.32054451, 0.34049905, 0.36045359, 0.38040813,\n",
       "        0.40036267, 0.42031721, 0.44027175, 0.4602263 , 0.48018084,\n",
       "        0.50013538, 0.52008992, 0.54004446, 0.559999  , 0.57995354,\n",
       "        0.59990808, 0.61986262, 0.63981716, 0.6597717 , 0.67972624,\n",
       "        0.69968078, 0.71963532, 0.73958986, 0.7595444 , 0.77949894,\n",
       "        0.79945348, 0.81940802, 0.83936256, 0.8593171 , 0.87927164,\n",
       "        0.89922618, 0.91918072, 0.93913526, 0.9590898 , 0.97904434,\n",
       "        0.99899888]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(adfuller_vals, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total(wmsfes):\n",
    "    scores = 0\n",
    "    alpha = 12\n",
    "    scores = []\n",
    "    for m in wmsfes:\n",
    "        score = (1.8 - 1.6 / (1 + np.exp(-alpha * m)))\n",
    "        scores.append(score)\n",
    "    return np.sum(scores) / len(wmsfes), scores\n",
    "\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, history, orig, pred, period=12):\n",
    "        assert period in [4,12]\n",
    "        self.history = history\n",
    "        self.orig = orig\n",
    "        self.pred = pred\n",
    "        self.period = period\n",
    "\n",
    "\n",
    "def wmsfe(tasks):\n",
    "    hm = np.array([len(t.orig) for t in tasks])\n",
    "    km = np.sum(hm)\n",
    "    scores = []\n",
    "    for t in tasks:\n",
    "        assert t is not None\n",
    "        assert t.orig.shape == t.pred.shape\n",
    "        disp = np.var(t.history[t.period:] - t.history[:-t.period])\n",
    "        disp = np.nan_to_num(disp) + 1e-9\n",
    "        score = np.sum((t.orig - t.pred) ** 2 / (np.arange(len(t.pred)) + 1)) / disp\n",
    "        scores.append(score)\n",
    "    return np.sum(scores) / km, scores / hm\n",
    "\n",
    "class TaskSet:\n",
    "    def __init__(self):\n",
    "        self.groups = []\n",
    "        \n",
    "    def buildFromTasks(self, tasks2d):\n",
    "        for g in tasks2d:\n",
    "            self.addGroup()\n",
    "            for t in g:\n",
    "                self.addTask(t)\n",
    "        return self\n",
    "\n",
    "    def addTask(self, task):\n",
    "        self.groups[-1].append(task)\n",
    "\n",
    "    def addGroup(self):\n",
    "        self.groups.append([])\n",
    "\n",
    "    def calcScore(self):\n",
    "        losses = []\n",
    "        for g in self.groups:\n",
    "            loss, _ = wmsfe(g)\n",
    "            losses.append(loss)\n",
    "        return total(losses)\n",
    "    \n",
    "\n",
    "def leaderboard(tasks2d):\n",
    "    return TaskSet().buildFromTasks(tasks2d).calcScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, period):\n",
    "        self.period = period\n",
    "        self.try_fallback = False\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = remove_nan_prefix(X)\n",
    "        l = X.shape[0]\n",
    "        if l < 2 * self.period:\n",
    "            assert False\n",
    "            ft = 0 if l == 0 else X[0]\n",
    "            X = np.array([float(ft)] * (3 * self.period - l) + list(X))\n",
    "            self.try_fallback = True\n",
    "        r = seasonal_decompose(X, model=\"additive\", period=self.period)\n",
    "        trend = X - r.seasonal\n",
    "        \n",
    "        mean = np.mean(trend)\n",
    "        std = np.std(trend)\n",
    "        trend_norm = (trend - mean) / (std + 1e-9)\n",
    "        \n",
    "        self.seasonal = r.seasonal\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "#         self.trend_model = trend_model\n",
    "        return trend_norm\n",
    "    def inverse(self, X):\n",
    "        X *= self.std\n",
    "        X += self.mean\n",
    "        seasons = np.tile(self.seasonal, len(X) // len(self.seasonal) + 1)[:len(X)]\n",
    "        X += seasons\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "class ZeroModel(BaseModel):\n",
    "    def predict(self,_):\n",
    "        return [0]\n",
    "\n",
    "class LastConstModel(BaseModel):\n",
    "    def predict(self, X):\n",
    "        return [X[0][-1]]\n",
    "    \n",
    "class FirstConstModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        self.const = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.const is None:\n",
    "            self.const = X[0][-1]\n",
    "        return [self.const]\n",
    "    \n",
    "class Weighted(BaseModel):\n",
    "    def __init__(self, weights, base_model):\n",
    "        self.weights = weights\n",
    "        self.base_model = base_model\n",
    "    def fit(self, X, y):\n",
    "        return self.base_model.fit(X * self.weights, y)\n",
    "    def predict(self, X):\n",
    "        return self.base_model.predict(X * self.weights)\n",
    "    \n",
    "class Scaled(BaseModel):\n",
    "    def __init__(self, weight, base_model):\n",
    "        self.weight = weight\n",
    "        self.base_model = base_model\n",
    "    def fit(self, X, y):\n",
    "        return self.base_model.fit(X, y)\n",
    "    def predict(self, X):\n",
    "        return self.base_model.predict(X) * self.weight\n",
    "    \n",
    "class PredictWithDelta(BaseModel):\n",
    "    def __init__(self, base_model):\n",
    "        self.base_model = base_model\n",
    "    def predict(self, X):\n",
    "        y = self.base_model.predict(X)\n",
    "        return X[:, -1] + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cumulative_noise(X, weight=1e-2):\n",
    "    assert len(X.shape) == 2 and isinstance(X, np.ndarray)\n",
    "    Z = np.random.normal(size=X.shape)\n",
    "    W = np.tile(np.arange(X.shape[0], 0, -1).reshape(-1,1), X.shape[1]) / X.shape[0] * weight\n",
    "    Z = np.cumsum(Z, axis=0) * W\n",
    "    return X * (1 + Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "withNoise = add_cumulative_noise(train_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8038f49f98>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 15\n",
    "plt.plot(train_data.values[:, T])\n",
    "plt.plot(withNoise[:, T], \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "class SearchSequenceIndex:\n",
    "    def __init__(self, window, period, offset):\n",
    "        self.window = window\n",
    "        self.period = period\n",
    "        self.offset = offset\n",
    "        \n",
    "    def get_train_part_and_full(self, X, remove_prefix=0):\n",
    "        train_gap = self.offset + self.period + self.window\n",
    "        X = X[remove_prefix:]\n",
    "        T = X[:-train_gap]\n",
    "        return T, X\n",
    "    \n",
    "    def build(self, X):\n",
    "        assert len(X.shape) == 2\n",
    "        self.X_normed = []\n",
    "\n",
    "        for i in np.arange(X.shape[1]):\n",
    "            x = X[:, i]\n",
    "            x = Normalizer(self.period).transform(x)\n",
    "            for j in range(len(x) - self.window - 1):\n",
    "                sample = x[j:j + self.window + 1]\n",
    "                self.X_normed.append(sample)\n",
    "        self.X_normed = np.array(self.X_normed)\n",
    "    \n",
    "    def fit(self, base_model):\n",
    "        self.base_model = base_model\n",
    "        X_train = np.array([x[:-1] for x in self.X_normed])\n",
    "        y_train = np.array([x[-1] - x[-2] for x in self.X_normed])\n",
    "        self.base_model.fit(X_train, y_train)\n",
    "        self.base_model = PredictWithDelta(self.base_model)\n",
    "        \n",
    "    def predict_for_seq(self, seq, steps):\n",
    "        assert len(seq.shape) == 1, seq.shape[0] >= 2 * self.window\n",
    "        normalizer = Normalizer(self.period)\n",
    "        seq = normalizer.transform(seq)\n",
    "        for i in np.arange(steps):\n",
    "            subSeq = seq[-self.window:]\n",
    "            origSubSeq = subSeq\n",
    "            x = subSeq.reshape(1,-1)\n",
    "            if normalizer.try_fallback:\n",
    "                y = PredictWithDelta(ZeroModel()).predict(x)[0]\n",
    "            else:\n",
    "                y = self.base_model.predict(x)[0]\n",
    "            subSeq = list(subSeq) + [y]\n",
    "            assert np.mean((subSeq[:-1] - origSubSeq) ** 2) < 1e-6, (subSeq, origSubSeq)\n",
    "            seq = np.append(seq, subSeq[-1])\n",
    "        seq = normalizer.inverse(seq)\n",
    "        return seq\n",
    "\n",
    "    def measure_on_all_train_data(self, X, offset):\n",
    "        mse = 0\n",
    "        N = len(X)\n",
    "        tasks = []\n",
    "        for t in np.arange(N):\n",
    "            history = remove_nan_prefix(X[t])\n",
    "            window_data = history[:-offset]\n",
    "            r = self.predict_for_seq(window_data, offset)\n",
    "            task = Task(window_data, history[-offset:], r[-offset:])\n",
    "            tasks.append(task)\n",
    "        return tasks\n",
    "    \n",
    "def index_series(X, window, period, model, step=1, noise_iters=0, noise_weight=1e-2, global_offset=15, sample=1):\n",
    "    np.random.seed(0)\n",
    "    assert len(X.shape) == 2 and isinstance(X, np.ndarray)\n",
    "    xlen = X.shape[0]\n",
    "    scores = []\n",
    "    losses = []\n",
    "    use_noise = True if noise_iters > 0 else False\n",
    "    noise_iters = max(1, noise_iters)\n",
    "    for t in range(noise_iters):\n",
    "        for offset in tqdm.tqdm(np.arange(0, xlen - 2 * (period + window + global_offset) - 1, step), position=0):\n",
    "            index = SearchSequenceIndex(window, period, global_offset)\n",
    "            T, F = index.get_train_part_and_full(X, offset)\n",
    "            if sample < 1:\n",
    "                T = T[:, np.random.choice(np.arange(T.shape[1]),size=int(T.shape[1] * sample), replace=False)]\n",
    "            if use_noise:\n",
    "                T = add_cumulative_noise(T, noise_weight)\n",
    "            index.build(T)\n",
    "            index.fit(model())\n",
    "            res = index.measure_on_all_train_data(np.transpose(F), global_offset)\n",
    "            mean_loss, _losses = wmsfe(res)\n",
    "            score, scores = total(_losses)\n",
    "            scores.extend(scores)\n",
    "            losses.extend(_losses)\n",
    "    return np.array(scores), np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 2])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1,2,3,4],size=3, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.78it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  7.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "zero_measures = index_series(\n",
    "    train_data.values,\n",
    "    window=12 * 3,\n",
    "    period=12,\n",
    "    model=lambda:ZeroModel(),\n",
    "    step=6,\n",
    "    noise_iters=3,\n",
    "    noise_weight=1e-3,\n",
    "    sample=0.75\n",
    ")\n",
    "\n",
    "knn_measures = index_series(\n",
    "    train_data.values,\n",
    "    window=12 * 3,\n",
    "    period=12,\n",
    "    model=lambda:Scaled(0.5, KNeighborsRegressor(n_neighbors=50)),\n",
    "    step=6,\n",
    "    noise_iters=3,\n",
    "    noise_weight=1e-3,\n",
    "    sample=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes = (windowm, 32, 16, 32, windowm), \n",
    "                   activation = 'tanh', \n",
    "                   solver = 'adam',\n",
    "                   batch_size=32,\n",
    "#                    learning_rate_init = 0.0001, \n",
    "#                    max_iter = 20, \n",
    "#                    tol = 0.0000001, \n",
    "                   verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(reg, data):\n",
    "    data = np.asmatrix(data)\n",
    "    \n",
    "    encoder1 = data*reg.coefs_[0] + reg.intercepts_[0]\n",
    "    encoder1 = (np.exp(encoder1) - np.exp(-encoder1))/(np.exp(encoder1) + np.exp(-encoder1))\n",
    "    encoder1 = 1 / (1 + np.exp(-encoder1))\n",
    "    \n",
    "    encoder2 = encoder1*reg.coefs_[1] + reg.intercepts_[1]\n",
    "    encoder2 = (np.exp(encoder2) - np.exp(-encoder2))/(np.exp(encoder2) + np.exp(-encoder2))\n",
    "    encoder1 = 1 / (1 + np.exp(-encoder1))\n",
    "    \n",
    "    latent = encoder2*reg.coefs_[2] + reg.intercepts_[2]\n",
    "    latent = (np.exp(latent) - np.exp(-latent))/(np.exp(latent) + np.exp(-latent))\n",
    "    \n",
    "    return np.asarray(latent)\n",
    "\n",
    "class AutoencKnnWrapper(BaseModel):\n",
    "    def __init__(self, enc, knn, err_bound=0):\n",
    "        self.enc = enc\n",
    "        self.knn = knn\n",
    "#         self.err_bound = err_bound\n",
    "    def fit(self, X, y):\n",
    "        ids = np.arange(len(X))\n",
    "        self.enc.fit(X, X)\n",
    "        X_enc = encoder(self.enc, X)\n",
    "#         print(X_enc.shape)\n",
    "        self.knn.fit(X_enc, y)\n",
    "    def predict(self, X):\n",
    "#         XT = self.enc.predict(X)\n",
    "#         diff = np.mean((XT - X) ** 2)\n",
    "#         if diff > self.err_bound:\n",
    "#             return np.array([0])\n",
    "        X_enc = encoder(self.enc, X)\n",
    "        return self.knn.predict(X_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                  | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.17231139\n",
      "Iteration 2, loss = 0.08288363\n",
      "Iteration 3, loss = 0.06149914\n",
      "Iteration 4, loss = 0.05100374\n",
      "Iteration 5, loss = 0.04541857\n",
      "Iteration 6, loss = 0.04180299\n",
      "Iteration 7, loss = 0.03916913\n",
      "Iteration 8, loss = 0.03719628\n",
      "Iteration 9, loss = 0.03550513\n",
      "Iteration 10, loss = 0.03433021\n",
      "Iteration 11, loss = 0.03305880\n",
      "Iteration 12, loss = 0.03202231\n",
      "Iteration 13, loss = 0.03106933\n",
      "Iteration 14, loss = 0.03016317\n",
      "Iteration 15, loss = 0.02934253\n",
      "Iteration 16, loss = 0.02871523\n",
      "Iteration 17, loss = 0.02808627\n",
      "Iteration 18, loss = 0.02757656\n",
      "Iteration 19, loss = 0.02705498\n",
      "Iteration 20, loss = 0.02656998\n",
      "Iteration 21, loss = 0.02615358\n",
      "Iteration 22, loss = 0.02579040\n",
      "Iteration 23, loss = 0.02559488\n",
      "Iteration 24, loss = 0.02521991\n",
      "Iteration 25, loss = 0.02478189\n",
      "Iteration 26, loss = 0.02438679\n",
      "Iteration 27, loss = 0.02427101\n",
      "Iteration 28, loss = 0.02392041\n",
      "Iteration 29, loss = 0.02356982\n",
      "Iteration 30, loss = 0.02329498\n",
      "Iteration 31, loss = 0.02319468\n",
      "Iteration 32, loss = 0.02297086\n",
      "Iteration 33, loss = 0.02256077\n",
      "Iteration 34, loss = 0.02241242\n",
      "Iteration 35, loss = 0.02221616\n",
      "Iteration 36, loss = 0.02190916\n",
      "Iteration 37, loss = 0.02165848\n",
      "Iteration 38, loss = 0.02152562\n",
      "Iteration 39, loss = 0.02134975\n",
      "Iteration 40, loss = 0.02117147\n",
      "Iteration 41, loss = 0.02105538\n",
      "Iteration 42, loss = 0.02084583\n",
      "Iteration 43, loss = 0.02079265\n",
      "Iteration 44, loss = 0.02056652\n",
      "Iteration 45, loss = 0.02037612\n",
      "Iteration 46, loss = 0.02022799\n",
      "Iteration 47, loss = 0.02018083\n",
      "Iteration 48, loss = 0.02005750\n",
      "Iteration 49, loss = 0.01991646\n",
      "Iteration 50, loss = 0.01989765\n",
      "Iteration 51, loss = 0.01981728\n",
      "Iteration 52, loss = 0.01963202\n",
      "Iteration 53, loss = 0.01954660\n",
      "Iteration 54, loss = 0.01956913\n",
      "Iteration 55, loss = 0.01941488\n",
      "Iteration 56, loss = 0.01931660\n",
      "Iteration 57, loss = 0.01926685\n",
      "Iteration 58, loss = 0.01924407\n",
      "Iteration 59, loss = 0.01920364\n",
      "Iteration 60, loss = 0.01916480\n",
      "Iteration 61, loss = 0.01903140\n",
      "Iteration 62, loss = 0.01890626\n",
      "Iteration 63, loss = 0.01899485\n",
      "Iteration 64, loss = 0.01897656\n",
      "Iteration 65, loss = 0.01889397\n",
      "Iteration 66, loss = 0.01875891\n",
      "Iteration 67, loss = 0.01881424\n",
      "Iteration 68, loss = 0.01882246\n",
      "Iteration 69, loss = 0.01880079\n",
      "Iteration 70, loss = 0.01860861\n",
      "Iteration 71, loss = 0.01861371\n",
      "Iteration 72, loss = 0.01854412\n",
      "Iteration 73, loss = 0.01858674\n",
      "Iteration 74, loss = 0.01846349\n",
      "Iteration 75, loss = 0.01850981\n",
      "Iteration 76, loss = 0.01851612\n",
      "Iteration 77, loss = 0.01848559\n",
      "Iteration 78, loss = 0.01840403\n",
      "Iteration 79, loss = 0.01840680\n",
      "Iteration 80, loss = 0.01830331\n",
      "Iteration 81, loss = 0.01846048\n",
      "Iteration 82, loss = 0.01836202\n",
      "Iteration 83, loss = 0.01821036\n",
      "Iteration 84, loss = 0.01821178\n",
      "Iteration 85, loss = 0.01815557\n",
      "Iteration 86, loss = 0.01817393\n",
      "Iteration 87, loss = 0.01822304\n",
      "Iteration 88, loss = 0.01825939\n",
      "Iteration 89, loss = 0.01815183\n",
      "Iteration 90, loss = 0.01806471\n",
      "Iteration 91, loss = 0.01813964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████▊                                                                                                                           | 1/5 [00:06<00:26,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.19281405\n",
      "Iteration 2, loss = 0.08997800\n",
      "Iteration 3, loss = 0.06818412\n",
      "Iteration 4, loss = 0.05555301\n",
      "Iteration 5, loss = 0.04873400\n",
      "Iteration 6, loss = 0.04423917\n",
      "Iteration 7, loss = 0.04100392\n",
      "Iteration 8, loss = 0.03835530\n",
      "Iteration 9, loss = 0.03648002\n",
      "Iteration 10, loss = 0.03490528\n",
      "Iteration 11, loss = 0.03345326\n",
      "Iteration 12, loss = 0.03235410\n",
      "Iteration 13, loss = 0.03132121\n",
      "Iteration 14, loss = 0.03049105\n",
      "Iteration 15, loss = 0.02965189\n",
      "Iteration 16, loss = 0.02910337\n",
      "Iteration 17, loss = 0.02827668\n",
      "Iteration 18, loss = 0.02773913\n",
      "Iteration 19, loss = 0.02720766\n",
      "Iteration 20, loss = 0.02669389\n",
      "Iteration 21, loss = 0.02611155\n",
      "Iteration 22, loss = 0.02565995\n",
      "Iteration 23, loss = 0.02526040\n",
      "Iteration 24, loss = 0.02479343\n",
      "Iteration 25, loss = 0.02453589\n",
      "Iteration 26, loss = 0.02410027\n",
      "Iteration 27, loss = 0.02394240\n",
      "Iteration 28, loss = 0.02335085\n",
      "Iteration 29, loss = 0.02306545\n",
      "Iteration 30, loss = 0.02270886\n",
      "Iteration 31, loss = 0.02249530\n",
      "Iteration 32, loss = 0.02219699\n",
      "Iteration 33, loss = 0.02196885\n",
      "Iteration 34, loss = 0.02186859\n",
      "Iteration 35, loss = 0.02152861\n",
      "Iteration 36, loss = 0.02141754\n",
      "Iteration 37, loss = 0.02105297\n",
      "Iteration 38, loss = 0.02086222\n",
      "Iteration 39, loss = 0.02074295\n",
      "Iteration 40, loss = 0.02062117\n",
      "Iteration 41, loss = 0.02060965\n",
      "Iteration 42, loss = 0.02026999\n",
      "Iteration 43, loss = 0.02013363\n",
      "Iteration 44, loss = 0.02004733\n",
      "Iteration 45, loss = 0.01999083\n",
      "Iteration 46, loss = 0.01986783\n",
      "Iteration 47, loss = 0.01973105\n",
      "Iteration 48, loss = 0.01966367\n",
      "Iteration 49, loss = 0.01969743\n",
      "Iteration 50, loss = 0.01963549\n",
      "Iteration 51, loss = 0.01931510\n",
      "Iteration 52, loss = 0.01915881\n",
      "Iteration 53, loss = 0.01914590\n",
      "Iteration 54, loss = 0.01908985\n",
      "Iteration 55, loss = 0.01900609\n",
      "Iteration 56, loss = 0.01895422\n",
      "Iteration 57, loss = 0.01892443\n",
      "Iteration 58, loss = 0.01878356\n",
      "Iteration 59, loss = 0.01862170\n",
      "Iteration 60, loss = 0.01873503\n",
      "Iteration 61, loss = 0.01848799\n",
      "Iteration 62, loss = 0.01844709\n",
      "Iteration 63, loss = 0.01835023\n",
      "Iteration 64, loss = 0.01842214\n",
      "Iteration 65, loss = 0.01840733\n",
      "Iteration 66, loss = 0.01816686\n",
      "Iteration 67, loss = 0.01812185\n",
      "Iteration 68, loss = 0.01804955\n",
      "Iteration 69, loss = 0.01818617\n",
      "Iteration 70, loss = 0.01806159\n",
      "Iteration 71, loss = 0.01810021\n",
      "Iteration 72, loss = 0.01794432\n",
      "Iteration 73, loss = 0.01792820\n",
      "Iteration 74, loss = 0.01780763\n",
      "Iteration 75, loss = 0.01778293\n",
      "Iteration 76, loss = 0.01775464\n",
      "Iteration 77, loss = 0.01791413\n",
      "Iteration 78, loss = 0.01781659\n",
      "Iteration 79, loss = 0.01763033\n",
      "Iteration 80, loss = 0.01765694\n",
      "Iteration 81, loss = 0.01768479\n",
      "Iteration 82, loss = 0.01759170\n",
      "Iteration 83, loss = 0.01750951\n",
      "Iteration 84, loss = 0.01749603\n",
      "Iteration 85, loss = 0.01750346\n",
      "Iteration 86, loss = 0.01748760\n",
      "Iteration 87, loss = 0.01749144\n",
      "Iteration 88, loss = 0.01746902\n",
      "Iteration 89, loss = 0.01761133\n",
      "Iteration 90, loss = 0.01757287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████████████████████████████▌                                                                                            | 2/5 [00:12<00:18,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.21488536\n",
      "Iteration 2, loss = 0.09611825\n",
      "Iteration 3, loss = 0.07051500\n",
      "Iteration 4, loss = 0.05875238\n",
      "Iteration 5, loss = 0.05224620\n",
      "Iteration 6, loss = 0.04762884\n",
      "Iteration 7, loss = 0.04414202\n",
      "Iteration 8, loss = 0.04129749\n",
      "Iteration 9, loss = 0.03934190\n",
      "Iteration 10, loss = 0.03738755\n",
      "Iteration 11, loss = 0.03601538\n",
      "Iteration 12, loss = 0.03483971\n",
      "Iteration 13, loss = 0.03363907\n",
      "Iteration 14, loss = 0.03263969\n",
      "Iteration 15, loss = 0.03162231\n",
      "Iteration 16, loss = 0.03071069\n",
      "Iteration 17, loss = 0.02983760\n",
      "Iteration 18, loss = 0.02908514\n",
      "Iteration 19, loss = 0.02831166\n",
      "Iteration 20, loss = 0.02766844\n",
      "Iteration 21, loss = 0.02712784\n",
      "Iteration 22, loss = 0.02661330\n",
      "Iteration 23, loss = 0.02632407\n",
      "Iteration 24, loss = 0.02582103\n",
      "Iteration 25, loss = 0.02548964\n",
      "Iteration 26, loss = 0.02526845\n",
      "Iteration 27, loss = 0.02482102\n",
      "Iteration 28, loss = 0.02460126\n",
      "Iteration 29, loss = 0.02417722\n",
      "Iteration 30, loss = 0.02392857\n",
      "Iteration 31, loss = 0.02367780\n",
      "Iteration 32, loss = 0.02337340\n",
      "Iteration 33, loss = 0.02319190\n",
      "Iteration 34, loss = 0.02289776\n",
      "Iteration 35, loss = 0.02281760\n",
      "Iteration 36, loss = 0.02255078\n",
      "Iteration 37, loss = 0.02226159\n",
      "Iteration 38, loss = 0.02194631\n",
      "Iteration 39, loss = 0.02185833\n",
      "Iteration 40, loss = 0.02163181\n",
      "Iteration 41, loss = 0.02134370\n",
      "Iteration 42, loss = 0.02108954\n",
      "Iteration 43, loss = 0.02109335\n",
      "Iteration 44, loss = 0.02068623\n",
      "Iteration 45, loss = 0.02050810\n",
      "Iteration 46, loss = 0.02040900\n",
      "Iteration 47, loss = 0.02026494\n",
      "Iteration 48, loss = 0.02033931\n",
      "Iteration 49, loss = 0.02016380\n",
      "Iteration 50, loss = 0.01986206\n",
      "Iteration 51, loss = 0.01974947\n",
      "Iteration 52, loss = 0.01970938\n",
      "Iteration 53, loss = 0.01980365\n",
      "Iteration 54, loss = 0.01948824\n",
      "Iteration 55, loss = 0.01939223\n",
      "Iteration 56, loss = 0.01932522\n",
      "Iteration 57, loss = 0.01926579\n",
      "Iteration 58, loss = 0.01910659\n",
      "Iteration 59, loss = 0.01919741\n",
      "Iteration 60, loss = 0.01909803\n",
      "Iteration 61, loss = 0.01900490\n",
      "Iteration 62, loss = 0.01888852\n",
      "Iteration 63, loss = 0.01876740\n",
      "Iteration 64, loss = 0.01874234\n",
      "Iteration 65, loss = 0.01859735\n",
      "Iteration 66, loss = 0.01877340\n",
      "Iteration 67, loss = 0.01860328\n",
      "Iteration 68, loss = 0.01845158\n",
      "Iteration 69, loss = 0.01852661\n",
      "Iteration 70, loss = 0.01846019\n",
      "Iteration 71, loss = 0.01852782\n",
      "Iteration 72, loss = 0.01828774\n",
      "Iteration 73, loss = 0.01836543\n",
      "Iteration 74, loss = 0.01830819\n",
      "Iteration 75, loss = 0.01816980\n",
      "Iteration 76, loss = 0.01814237\n",
      "Iteration 77, loss = 0.01812443\n",
      "Iteration 78, loss = 0.01800404\n",
      "Iteration 79, loss = 0.01803750\n",
      "Iteration 80, loss = 0.01804624\n",
      "Iteration 81, loss = 0.01791824\n",
      "Iteration 82, loss = 0.01786866\n",
      "Iteration 83, loss = 0.01798057\n",
      "Iteration 84, loss = 0.01786238\n",
      "Iteration 85, loss = 0.01785774\n",
      "Iteration 86, loss = 0.01781766\n",
      "Iteration 87, loss = 0.01775298\n",
      "Iteration 88, loss = 0.01778307\n",
      "Iteration 89, loss = 0.01780115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 3/5 [00:17<00:11,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22921111\n",
      "Iteration 2, loss = 0.09919835\n",
      "Iteration 3, loss = 0.06804691\n",
      "Iteration 4, loss = 0.05503237\n",
      "Iteration 5, loss = 0.04822801\n",
      "Iteration 6, loss = 0.04320438\n",
      "Iteration 7, loss = 0.03962120\n",
      "Iteration 8, loss = 0.03675187\n",
      "Iteration 9, loss = 0.03479981\n",
      "Iteration 10, loss = 0.03305802\n",
      "Iteration 11, loss = 0.03190774\n",
      "Iteration 12, loss = 0.03072114\n",
      "Iteration 13, loss = 0.02974485\n",
      "Iteration 14, loss = 0.02902334\n",
      "Iteration 15, loss = 0.02798932\n",
      "Iteration 16, loss = 0.02727955\n",
      "Iteration 17, loss = 0.02656411\n",
      "Iteration 18, loss = 0.02596220\n",
      "Iteration 19, loss = 0.02545812\n",
      "Iteration 20, loss = 0.02485483\n",
      "Iteration 21, loss = 0.02458389\n",
      "Iteration 22, loss = 0.02398035\n",
      "Iteration 23, loss = 0.02355332\n",
      "Iteration 24, loss = 0.02314388\n",
      "Iteration 25, loss = 0.02273593\n",
      "Iteration 26, loss = 0.02248750\n",
      "Iteration 27, loss = 0.02226603\n",
      "Iteration 28, loss = 0.02191252\n",
      "Iteration 29, loss = 0.02151024\n",
      "Iteration 30, loss = 0.02117085\n",
      "Iteration 31, loss = 0.02097306\n",
      "Iteration 32, loss = 0.02079850\n",
      "Iteration 33, loss = 0.02052795\n",
      "Iteration 34, loss = 0.02022826\n",
      "Iteration 35, loss = 0.01995175\n",
      "Iteration 36, loss = 0.01977030\n",
      "Iteration 37, loss = 0.01966259\n",
      "Iteration 38, loss = 0.01939062\n",
      "Iteration 39, loss = 0.01909888\n",
      "Iteration 40, loss = 0.01894045\n",
      "Iteration 41, loss = 0.01879843\n",
      "Iteration 42, loss = 0.01860779\n",
      "Iteration 43, loss = 0.01836795\n",
      "Iteration 44, loss = 0.01818864\n",
      "Iteration 45, loss = 0.01808899\n",
      "Iteration 46, loss = 0.01805876\n",
      "Iteration 47, loss = 0.01784971\n",
      "Iteration 48, loss = 0.01758747\n",
      "Iteration 49, loss = 0.01759025\n",
      "Iteration 50, loss = 0.01747862\n",
      "Iteration 51, loss = 0.01749867\n",
      "Iteration 52, loss = 0.01730475\n",
      "Iteration 53, loss = 0.01706982\n",
      "Iteration 54, loss = 0.01695756\n",
      "Iteration 55, loss = 0.01697438\n",
      "Iteration 56, loss = 0.01677356\n",
      "Iteration 57, loss = 0.01675797\n",
      "Iteration 58, loss = 0.01667569\n",
      "Iteration 59, loss = 0.01656930\n",
      "Iteration 60, loss = 0.01652438\n",
      "Iteration 61, loss = 0.01646126\n",
      "Iteration 62, loss = 0.01630870\n",
      "Iteration 63, loss = 0.01620571\n",
      "Iteration 64, loss = 0.01615296\n",
      "Iteration 65, loss = 0.01608699\n",
      "Iteration 66, loss = 0.01622301\n",
      "Iteration 67, loss = 0.01591178\n",
      "Iteration 68, loss = 0.01595611\n",
      "Iteration 69, loss = 0.01597751\n",
      "Iteration 70, loss = 0.01571504\n",
      "Iteration 71, loss = 0.01576390\n",
      "Iteration 72, loss = 0.01565315\n",
      "Iteration 73, loss = 0.01575831\n",
      "Iteration 74, loss = 0.01545141\n",
      "Iteration 75, loss = 0.01544595\n",
      "Iteration 76, loss = 0.01546263\n",
      "Iteration 77, loss = 0.01550436\n",
      "Iteration 78, loss = 0.01536488\n",
      "Iteration 79, loss = 0.01527598\n",
      "Iteration 80, loss = 0.01522338\n",
      "Iteration 81, loss = 0.01518453\n",
      "Iteration 82, loss = 0.01522021\n",
      "Iteration 83, loss = 0.01510237\n",
      "Iteration 84, loss = 0.01507323\n",
      "Iteration 85, loss = 0.01502836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 4/5 [00:22<00:05,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.29054167\n",
      "Iteration 2, loss = 0.12025475\n",
      "Iteration 3, loss = 0.08626525\n",
      "Iteration 4, loss = 0.06849847\n",
      "Iteration 5, loss = 0.05818265\n",
      "Iteration 6, loss = 0.05174371\n",
      "Iteration 7, loss = 0.04738515\n",
      "Iteration 8, loss = 0.04353065\n",
      "Iteration 9, loss = 0.04061155\n",
      "Iteration 10, loss = 0.03815533\n",
      "Iteration 11, loss = 0.03622769\n",
      "Iteration 12, loss = 0.03480118\n",
      "Iteration 13, loss = 0.03342760\n",
      "Iteration 14, loss = 0.03234184\n",
      "Iteration 15, loss = 0.03137038\n",
      "Iteration 16, loss = 0.03057300\n",
      "Iteration 17, loss = 0.03004733\n",
      "Iteration 18, loss = 0.02912927\n",
      "Iteration 19, loss = 0.02849465\n",
      "Iteration 20, loss = 0.02797654\n",
      "Iteration 21, loss = 0.02745429\n",
      "Iteration 22, loss = 0.02701743\n",
      "Iteration 23, loss = 0.02647939\n",
      "Iteration 24, loss = 0.02591264\n",
      "Iteration 25, loss = 0.02553269\n",
      "Iteration 26, loss = 0.02507907\n",
      "Iteration 27, loss = 0.02458980\n",
      "Iteration 28, loss = 0.02421668\n",
      "Iteration 29, loss = 0.02386286\n",
      "Iteration 30, loss = 0.02343201\n",
      "Iteration 31, loss = 0.02316395\n",
      "Iteration 32, loss = 0.02295909\n",
      "Iteration 33, loss = 0.02257606\n",
      "Iteration 34, loss = 0.02235482\n",
      "Iteration 35, loss = 0.02211907\n",
      "Iteration 36, loss = 0.02181255\n",
      "Iteration 37, loss = 0.02151451\n",
      "Iteration 38, loss = 0.02143583\n",
      "Iteration 39, loss = 0.02111545\n",
      "Iteration 40, loss = 0.02091079\n",
      "Iteration 41, loss = 0.02071578\n",
      "Iteration 42, loss = 0.02055800\n",
      "Iteration 43, loss = 0.02034232\n",
      "Iteration 44, loss = 0.02015216\n",
      "Iteration 45, loss = 0.02001325\n",
      "Iteration 46, loss = 0.01973860\n",
      "Iteration 47, loss = 0.01961611\n",
      "Iteration 48, loss = 0.01956290\n",
      "Iteration 49, loss = 0.01938493\n",
      "Iteration 50, loss = 0.01919761\n",
      "Iteration 51, loss = 0.01916236\n",
      "Iteration 52, loss = 0.01891649\n",
      "Iteration 53, loss = 0.01880096\n",
      "Iteration 54, loss = 0.01864958\n",
      "Iteration 55, loss = 0.01852403\n",
      "Iteration 56, loss = 0.01846483\n",
      "Iteration 57, loss = 0.01837329\n",
      "Iteration 58, loss = 0.01842371\n",
      "Iteration 59, loss = 0.01830212\n",
      "Iteration 60, loss = 0.01791368\n",
      "Iteration 61, loss = 0.01774324\n",
      "Iteration 62, loss = 0.01761176\n",
      "Iteration 63, loss = 0.01752577\n",
      "Iteration 64, loss = 0.01745334\n",
      "Iteration 65, loss = 0.01727128\n",
      "Iteration 66, loss = 0.01739842\n",
      "Iteration 67, loss = 0.01699954\n",
      "Iteration 68, loss = 0.01690929\n",
      "Iteration 69, loss = 0.01682274\n",
      "Iteration 70, loss = 0.01681841\n",
      "Iteration 71, loss = 0.01667233\n",
      "Iteration 72, loss = 0.01657002\n",
      "Iteration 73, loss = 0.01655953\n",
      "Iteration 74, loss = 0.01653566\n",
      "Iteration 75, loss = 0.01642558\n",
      "Iteration 76, loss = 0.01624925\n",
      "Iteration 77, loss = 0.01611680\n",
      "Iteration 78, loss = 0.01601751\n",
      "Iteration 79, loss = 0.01609739\n",
      "Iteration 80, loss = 0.01594446\n",
      "Iteration 81, loss = 0.01601423\n",
      "Iteration 82, loss = 0.01598851\n",
      "Iteration 83, loss = 0.01580523\n",
      "Iteration 84, loss = 0.01573799\n",
      "Iteration 85, loss = 0.01567686\n",
      "Iteration 86, loss = 0.01555363\n",
      "Iteration 87, loss = 0.01550058\n",
      "Iteration 88, loss = 0.01546049\n",
      "Iteration 89, loss = 0.01542855\n",
      "Iteration 90, loss = 0.01534996\n",
      "Iteration 91, loss = 0.01545586\n",
      "Iteration 92, loss = 0.01531260\n",
      "Iteration 93, loss = 0.01523766\n",
      "Iteration 94, loss = 0.01518822\n",
      "Iteration 95, loss = 0.01527433\n",
      "Iteration 96, loss = 0.01506415\n",
      "Iteration 97, loss = 0.01515628\n",
      "Iteration 98, loss = 0.01509609\n",
      "Iteration 99, loss = 0.01500546\n",
      "Iteration 100, loss = 0.01500129\n",
      "Iteration 101, loss = 0.01489634\n",
      "Iteration 102, loss = 0.01486165\n",
      "Iteration 103, loss = 0.01486815\n",
      "Iteration 104, loss = 0.01490967\n",
      "Iteration 105, loss = 0.01473526\n",
      "Iteration 106, loss = 0.01477137\n",
      "Iteration 107, loss = 0.01467274\n",
      "Iteration 108, loss = 0.01474792\n",
      "Iteration 109, loss = 0.01467758\n",
      "Iteration 110, loss = 0.01466163\n",
      "Iteration 111, loss = 0.01461331\n",
      "Iteration 112, loss = 0.01460420\n",
      "Iteration 113, loss = 0.01451392\n",
      "Iteration 114, loss = 0.01452962\n",
      "Iteration 115, loss = 0.01447766\n",
      "Iteration 116, loss = 0.01440115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:27<00:00,  5.59s/it]\n",
      "  0%|                                                                                                                                                                  | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.17915981\n",
      "Iteration 2, loss = 0.07908921\n",
      "Iteration 3, loss = 0.05502234\n",
      "Iteration 4, loss = 0.04671695\n",
      "Iteration 5, loss = 0.04206683\n",
      "Iteration 6, loss = 0.03904884\n",
      "Iteration 7, loss = 0.03664486\n",
      "Iteration 8, loss = 0.03453206\n",
      "Iteration 9, loss = 0.03293729\n",
      "Iteration 10, loss = 0.03149204\n",
      "Iteration 11, loss = 0.03023337\n",
      "Iteration 12, loss = 0.02927148\n",
      "Iteration 13, loss = 0.02837087\n",
      "Iteration 14, loss = 0.02756899\n",
      "Iteration 15, loss = 0.02670826\n",
      "Iteration 16, loss = 0.02611389\n",
      "Iteration 17, loss = 0.02537833\n",
      "Iteration 18, loss = 0.02485037\n",
      "Iteration 19, loss = 0.02438996\n",
      "Iteration 20, loss = 0.02395335\n",
      "Iteration 21, loss = 0.02340302\n",
      "Iteration 22, loss = 0.02311757\n",
      "Iteration 23, loss = 0.02262914\n",
      "Iteration 24, loss = 0.02216769\n",
      "Iteration 25, loss = 0.02185641\n",
      "Iteration 26, loss = 0.02170849\n",
      "Iteration 27, loss = 0.02153209\n",
      "Iteration 28, loss = 0.02107630\n",
      "Iteration 29, loss = 0.02083637\n",
      "Iteration 30, loss = 0.02066518\n",
      "Iteration 31, loss = 0.02047570\n",
      "Iteration 32, loss = 0.02024746\n",
      "Iteration 33, loss = 0.02007769\n",
      "Iteration 34, loss = 0.01981032\n",
      "Iteration 35, loss = 0.01971261\n",
      "Iteration 36, loss = 0.01941957\n",
      "Iteration 37, loss = 0.01921871\n",
      "Iteration 38, loss = 0.01901568\n",
      "Iteration 39, loss = 0.01884327\n",
      "Iteration 40, loss = 0.01860490\n",
      "Iteration 41, loss = 0.01842456\n",
      "Iteration 42, loss = 0.01833840\n",
      "Iteration 43, loss = 0.01813481\n",
      "Iteration 44, loss = 0.01800358\n",
      "Iteration 45, loss = 0.01803584\n",
      "Iteration 46, loss = 0.01787485\n",
      "Iteration 47, loss = 0.01772317\n",
      "Iteration 48, loss = 0.01769148\n",
      "Iteration 49, loss = 0.01767353\n",
      "Iteration 50, loss = 0.01754040\n",
      "Iteration 51, loss = 0.01743595\n",
      "Iteration 52, loss = 0.01746443\n",
      "Iteration 53, loss = 0.01738510\n",
      "Iteration 54, loss = 0.01727953\n",
      "Iteration 55, loss = 0.01725367\n",
      "Iteration 56, loss = 0.01710693\n",
      "Iteration 57, loss = 0.01704053\n",
      "Iteration 58, loss = 0.01703492\n",
      "Iteration 59, loss = 0.01699227\n",
      "Iteration 60, loss = 0.01700514\n",
      "Iteration 61, loss = 0.01693073\n",
      "Iteration 62, loss = 0.01689544\n",
      "Iteration 63, loss = 0.01677810\n",
      "Iteration 64, loss = 0.01682446\n",
      "Iteration 65, loss = 0.01677814\n",
      "Iteration 66, loss = 0.01670721\n",
      "Iteration 67, loss = 0.01668692\n",
      "Iteration 68, loss = 0.01661260\n",
      "Iteration 69, loss = 0.01664234\n",
      "Iteration 70, loss = 0.01656773\n",
      "Iteration 71, loss = 0.01652999\n",
      "Iteration 72, loss = 0.01654041\n",
      "Iteration 73, loss = 0.01652326\n",
      "Iteration 74, loss = 0.01663531\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████▊                                                                                                                           | 1/5 [00:05<00:22,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.19994581\n",
      "Iteration 2, loss = 0.08119586\n",
      "Iteration 3, loss = 0.06198574\n",
      "Iteration 4, loss = 0.05289807\n",
      "Iteration 5, loss = 0.04718109\n",
      "Iteration 6, loss = 0.04248976\n",
      "Iteration 7, loss = 0.03885880\n",
      "Iteration 8, loss = 0.03611542\n",
      "Iteration 9, loss = 0.03413306\n",
      "Iteration 10, loss = 0.03270885\n",
      "Iteration 11, loss = 0.03150295\n",
      "Iteration 12, loss = 0.03046768\n",
      "Iteration 13, loss = 0.02954204\n",
      "Iteration 14, loss = 0.02871775\n",
      "Iteration 15, loss = 0.02799617\n",
      "Iteration 16, loss = 0.02746675\n",
      "Iteration 17, loss = 0.02672274\n",
      "Iteration 18, loss = 0.02626484\n",
      "Iteration 19, loss = 0.02567166\n",
      "Iteration 20, loss = 0.02504132\n",
      "Iteration 21, loss = 0.02452689\n",
      "Iteration 22, loss = 0.02424592\n",
      "Iteration 23, loss = 0.02376501\n",
      "Iteration 24, loss = 0.02333578\n",
      "Iteration 25, loss = 0.02292097\n",
      "Iteration 26, loss = 0.02264217\n",
      "Iteration 27, loss = 0.02239848\n",
      "Iteration 28, loss = 0.02209947\n",
      "Iteration 29, loss = 0.02181369\n",
      "Iteration 30, loss = 0.02157233\n",
      "Iteration 31, loss = 0.02113246\n",
      "Iteration 32, loss = 0.02086713\n",
      "Iteration 33, loss = 0.02068139\n",
      "Iteration 34, loss = 0.02042837\n",
      "Iteration 35, loss = 0.02029312\n",
      "Iteration 36, loss = 0.01997214\n",
      "Iteration 37, loss = 0.01992954\n",
      "Iteration 38, loss = 0.01983108\n",
      "Iteration 39, loss = 0.01955971\n",
      "Iteration 40, loss = 0.01936427\n",
      "Iteration 41, loss = 0.01923821\n",
      "Iteration 42, loss = 0.01907620\n",
      "Iteration 43, loss = 0.01899709\n",
      "Iteration 44, loss = 0.01883508\n",
      "Iteration 45, loss = 0.01871363\n",
      "Iteration 46, loss = 0.01880918\n",
      "Iteration 47, loss = 0.01860364\n",
      "Iteration 48, loss = 0.01833899\n",
      "Iteration 49, loss = 0.01825063\n",
      "Iteration 50, loss = 0.01814208\n",
      "Iteration 51, loss = 0.01808171\n",
      "Iteration 52, loss = 0.01794073\n",
      "Iteration 53, loss = 0.01781507\n",
      "Iteration 54, loss = 0.01781954\n",
      "Iteration 55, loss = 0.01761960\n",
      "Iteration 56, loss = 0.01752761\n",
      "Iteration 57, loss = 0.01752896\n",
      "Iteration 58, loss = 0.01747001\n",
      "Iteration 59, loss = 0.01742863\n",
      "Iteration 60, loss = 0.01731344\n",
      "Iteration 61, loss = 0.01741611\n",
      "Iteration 62, loss = 0.01726556\n",
      "Iteration 63, loss = 0.01720896\n",
      "Iteration 64, loss = 0.01713452\n",
      "Iteration 65, loss = 0.01718761\n",
      "Iteration 66, loss = 0.01701635\n",
      "Iteration 67, loss = 0.01692704\n",
      "Iteration 68, loss = 0.01690127\n",
      "Iteration 69, loss = 0.01681979\n",
      "Iteration 70, loss = 0.01701848\n",
      "Iteration 71, loss = 0.01677079\n",
      "Iteration 72, loss = 0.01680592\n",
      "Iteration 73, loss = 0.01676226\n",
      "Iteration 74, loss = 0.01671595\n",
      "Iteration 75, loss = 0.01664534\n",
      "Iteration 76, loss = 0.01659666\n",
      "Iteration 77, loss = 0.01658265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████████████████████████████▌                                                                                            | 2/5 [00:11<00:16,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22011733\n",
      "Iteration 2, loss = 0.09222978\n",
      "Iteration 3, loss = 0.06598429\n",
      "Iteration 4, loss = 0.05526556\n",
      "Iteration 5, loss = 0.04907852\n",
      "Iteration 6, loss = 0.04440623\n",
      "Iteration 7, loss = 0.04057248\n",
      "Iteration 8, loss = 0.03782052\n",
      "Iteration 9, loss = 0.03569613\n",
      "Iteration 10, loss = 0.03399839\n",
      "Iteration 11, loss = 0.03263019\n",
      "Iteration 12, loss = 0.03134119\n",
      "Iteration 13, loss = 0.03049212\n",
      "Iteration 14, loss = 0.02942738\n",
      "Iteration 15, loss = 0.02876572\n",
      "Iteration 16, loss = 0.02804900\n",
      "Iteration 17, loss = 0.02727866\n",
      "Iteration 18, loss = 0.02663499\n",
      "Iteration 19, loss = 0.02599270\n",
      "Iteration 20, loss = 0.02556701\n",
      "Iteration 21, loss = 0.02505213\n",
      "Iteration 22, loss = 0.02451077\n",
      "Iteration 23, loss = 0.02415052\n",
      "Iteration 24, loss = 0.02375974\n",
      "Iteration 25, loss = 0.02336784\n",
      "Iteration 26, loss = 0.02307071\n",
      "Iteration 27, loss = 0.02273248\n",
      "Iteration 28, loss = 0.02244417\n",
      "Iteration 29, loss = 0.02205680\n",
      "Iteration 30, loss = 0.02189049\n",
      "Iteration 31, loss = 0.02162447\n",
      "Iteration 32, loss = 0.02143365\n",
      "Iteration 33, loss = 0.02113897\n",
      "Iteration 34, loss = 0.02095966\n",
      "Iteration 35, loss = 0.02077327\n",
      "Iteration 36, loss = 0.02055854\n",
      "Iteration 37, loss = 0.02036695\n",
      "Iteration 38, loss = 0.02027492\n",
      "Iteration 39, loss = 0.02018617\n",
      "Iteration 40, loss = 0.02000089\n",
      "Iteration 41, loss = 0.01973010\n",
      "Iteration 42, loss = 0.01962540\n",
      "Iteration 43, loss = 0.01946372\n",
      "Iteration 44, loss = 0.01944919\n",
      "Iteration 45, loss = 0.01927773\n",
      "Iteration 46, loss = 0.01923747\n",
      "Iteration 47, loss = 0.01899097\n",
      "Iteration 48, loss = 0.01883448\n",
      "Iteration 49, loss = 0.01874288\n",
      "Iteration 50, loss = 0.01872102\n",
      "Iteration 51, loss = 0.01860545\n",
      "Iteration 52, loss = 0.01847353\n",
      "Iteration 53, loss = 0.01826049\n",
      "Iteration 54, loss = 0.01806463\n",
      "Iteration 55, loss = 0.01780340\n",
      "Iteration 56, loss = 0.01771130\n",
      "Iteration 57, loss = 0.01773192\n",
      "Iteration 58, loss = 0.01744610\n",
      "Iteration 59, loss = 0.01738203\n",
      "Iteration 60, loss = 0.01718244\n",
      "Iteration 61, loss = 0.01719665\n",
      "Iteration 62, loss = 0.01710263\n",
      "Iteration 63, loss = 0.01702987\n",
      "Iteration 64, loss = 0.01704250\n",
      "Iteration 65, loss = 0.01687210\n",
      "Iteration 66, loss = 0.01683159\n",
      "Iteration 67, loss = 0.01674755\n",
      "Iteration 68, loss = 0.01658162\n",
      "Iteration 69, loss = 0.01655719\n",
      "Iteration 70, loss = 0.01685467\n",
      "Iteration 71, loss = 0.01650353\n",
      "Iteration 72, loss = 0.01639397\n",
      "Iteration 73, loss = 0.01633578\n",
      "Iteration 74, loss = 0.01629381\n",
      "Iteration 75, loss = 0.01642063\n",
      "Iteration 76, loss = 0.01620090\n",
      "Iteration 77, loss = 0.01607467\n",
      "Iteration 78, loss = 0.01608015\n",
      "Iteration 79, loss = 0.01606194\n",
      "Iteration 80, loss = 0.01607710\n",
      "Iteration 81, loss = 0.01598720\n",
      "Iteration 82, loss = 0.01593112\n",
      "Iteration 83, loss = 0.01583711\n",
      "Iteration 84, loss = 0.01583957\n",
      "Iteration 85, loss = 0.01582694\n",
      "Iteration 86, loss = 0.01579068\n",
      "Iteration 87, loss = 0.01569763\n",
      "Iteration 88, loss = 0.01570129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 3/5 [00:16<00:10,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.24721304\n",
      "Iteration 2, loss = 0.10043841\n",
      "Iteration 3, loss = 0.07356576\n",
      "Iteration 4, loss = 0.06179314\n",
      "Iteration 5, loss = 0.05512960\n",
      "Iteration 6, loss = 0.05055097\n",
      "Iteration 7, loss = 0.04713533\n",
      "Iteration 8, loss = 0.04435440\n",
      "Iteration 9, loss = 0.04215180\n",
      "Iteration 10, loss = 0.04010909\n",
      "Iteration 11, loss = 0.03856456\n",
      "Iteration 12, loss = 0.03718723\n",
      "Iteration 13, loss = 0.03594940\n",
      "Iteration 14, loss = 0.03495903\n",
      "Iteration 15, loss = 0.03376874\n",
      "Iteration 16, loss = 0.03291049\n",
      "Iteration 17, loss = 0.03204585\n",
      "Iteration 18, loss = 0.03121564\n",
      "Iteration 19, loss = 0.03073648\n",
      "Iteration 20, loss = 0.02991881\n",
      "Iteration 21, loss = 0.02928563\n",
      "Iteration 22, loss = 0.02874417\n",
      "Iteration 23, loss = 0.02818995\n",
      "Iteration 24, loss = 0.02781338\n",
      "Iteration 25, loss = 0.02711982\n",
      "Iteration 26, loss = 0.02688282\n",
      "Iteration 27, loss = 0.02636901\n",
      "Iteration 28, loss = 0.02611249\n",
      "Iteration 29, loss = 0.02566509\n",
      "Iteration 30, loss = 0.02527234\n",
      "Iteration 31, loss = 0.02501444\n",
      "Iteration 32, loss = 0.02469683\n",
      "Iteration 33, loss = 0.02464239\n",
      "Iteration 34, loss = 0.02419755\n",
      "Iteration 35, loss = 0.02381240\n",
      "Iteration 36, loss = 0.02353110\n",
      "Iteration 37, loss = 0.02321520\n",
      "Iteration 38, loss = 0.02316907\n",
      "Iteration 39, loss = 0.02265744\n",
      "Iteration 40, loss = 0.02250437\n",
      "Iteration 41, loss = 0.02232027\n",
      "Iteration 42, loss = 0.02217410\n",
      "Iteration 43, loss = 0.02181392\n",
      "Iteration 44, loss = 0.02164336\n",
      "Iteration 45, loss = 0.02159908\n",
      "Iteration 46, loss = 0.02134812\n",
      "Iteration 47, loss = 0.02114163\n",
      "Iteration 48, loss = 0.02102024\n",
      "Iteration 49, loss = 0.02121381\n",
      "Iteration 50, loss = 0.02086644\n",
      "Iteration 51, loss = 0.02068952\n",
      "Iteration 52, loss = 0.02062173\n",
      "Iteration 53, loss = 0.02041380\n",
      "Iteration 54, loss = 0.02038586\n",
      "Iteration 55, loss = 0.02032947\n",
      "Iteration 56, loss = 0.02010194\n",
      "Iteration 57, loss = 0.02016538\n",
      "Iteration 58, loss = 0.02001446\n",
      "Iteration 59, loss = 0.01988681\n",
      "Iteration 60, loss = 0.01970966\n",
      "Iteration 61, loss = 0.01966156\n",
      "Iteration 62, loss = 0.01974471\n",
      "Iteration 63, loss = 0.01963075\n",
      "Iteration 64, loss = 0.01945728\n",
      "Iteration 65, loss = 0.01940428\n",
      "Iteration 66, loss = 0.01936434\n",
      "Iteration 67, loss = 0.01945197\n",
      "Iteration 68, loss = 0.01931553\n",
      "Iteration 69, loss = 0.01928572\n",
      "Iteration 70, loss = 0.01915593\n",
      "Iteration 71, loss = 0.01904770\n",
      "Iteration 72, loss = 0.01910114\n",
      "Iteration 73, loss = 0.01909742\n",
      "Iteration 74, loss = 0.01904070\n",
      "Iteration 75, loss = 0.01890141\n",
      "Iteration 76, loss = 0.01881560\n",
      "Iteration 77, loss = 0.01882654\n",
      "Iteration 78, loss = 0.01873302\n",
      "Iteration 79, loss = 0.01866944\n",
      "Iteration 80, loss = 0.01861327\n",
      "Iteration 81, loss = 0.01885471\n",
      "Iteration 82, loss = 0.01862234\n",
      "Iteration 83, loss = 0.01855014\n",
      "Iteration 84, loss = 0.01860630\n",
      "Iteration 85, loss = 0.01843728\n",
      "Iteration 86, loss = 0.01840723\n",
      "Iteration 87, loss = 0.01838779\n",
      "Iteration 88, loss = 0.01840031\n",
      "Iteration 89, loss = 0.01850852\n",
      "Iteration 90, loss = 0.01845177\n",
      "Iteration 91, loss = 0.01840483\n",
      "Iteration 92, loss = 0.01828565\n",
      "Iteration 93, loss = 0.01821053\n",
      "Iteration 94, loss = 0.01822203\n",
      "Iteration 95, loss = 0.01821569\n",
      "Iteration 96, loss = 0.01820444\n",
      "Iteration 97, loss = 0.01817887\n",
      "Iteration 98, loss = 0.01821875\n",
      "Iteration 99, loss = 0.01816839\n",
      "Iteration 100, loss = 0.01807254\n",
      "Iteration 101, loss = 0.01810202\n",
      "Iteration 102, loss = 0.01804634\n",
      "Iteration 103, loss = 0.01800734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 4/5 [00:21<00:05,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.25941031\n",
      "Iteration 2, loss = 0.11387228\n",
      "Iteration 3, loss = 0.08141314\n",
      "Iteration 4, loss = 0.06586507\n",
      "Iteration 5, loss = 0.05822308\n",
      "Iteration 6, loss = 0.05371764\n",
      "Iteration 7, loss = 0.05037101\n",
      "Iteration 8, loss = 0.04771748\n",
      "Iteration 9, loss = 0.04555475\n",
      "Iteration 10, loss = 0.04334571\n",
      "Iteration 11, loss = 0.04148181\n",
      "Iteration 12, loss = 0.03972889\n",
      "Iteration 13, loss = 0.03835264\n",
      "Iteration 14, loss = 0.03697332\n",
      "Iteration 15, loss = 0.03576368\n",
      "Iteration 16, loss = 0.03491346\n",
      "Iteration 17, loss = 0.03370839\n",
      "Iteration 18, loss = 0.03292615\n",
      "Iteration 19, loss = 0.03211462\n",
      "Iteration 20, loss = 0.03139024\n",
      "Iteration 21, loss = 0.03063691\n",
      "Iteration 22, loss = 0.03008467\n",
      "Iteration 23, loss = 0.02933784\n",
      "Iteration 24, loss = 0.02880383\n",
      "Iteration 25, loss = 0.02836888\n",
      "Iteration 26, loss = 0.02780420\n",
      "Iteration 27, loss = 0.02731513\n",
      "Iteration 28, loss = 0.02695185\n",
      "Iteration 29, loss = 0.02641333\n",
      "Iteration 30, loss = 0.02606476\n",
      "Iteration 31, loss = 0.02560020\n",
      "Iteration 32, loss = 0.02520455\n",
      "Iteration 33, loss = 0.02500434\n",
      "Iteration 34, loss = 0.02457743\n",
      "Iteration 35, loss = 0.02420367\n",
      "Iteration 36, loss = 0.02404643\n",
      "Iteration 37, loss = 0.02381181\n",
      "Iteration 38, loss = 0.02359831\n",
      "Iteration 39, loss = 0.02324291\n",
      "Iteration 40, loss = 0.02290044\n",
      "Iteration 41, loss = 0.02277294\n",
      "Iteration 42, loss = 0.02246290\n",
      "Iteration 43, loss = 0.02223746\n",
      "Iteration 44, loss = 0.02197788\n",
      "Iteration 45, loss = 0.02191966\n",
      "Iteration 46, loss = 0.02178082\n",
      "Iteration 47, loss = 0.02146637\n",
      "Iteration 48, loss = 0.02130441\n",
      "Iteration 49, loss = 0.02112606\n",
      "Iteration 50, loss = 0.02106346\n",
      "Iteration 51, loss = 0.02106445\n",
      "Iteration 52, loss = 0.02082707\n",
      "Iteration 53, loss = 0.02055627\n",
      "Iteration 54, loss = 0.02057780\n",
      "Iteration 55, loss = 0.02037958\n",
      "Iteration 56, loss = 0.02025482\n",
      "Iteration 57, loss = 0.02026448\n",
      "Iteration 58, loss = 0.02029593\n",
      "Iteration 59, loss = 0.01988533\n",
      "Iteration 60, loss = 0.01986117\n",
      "Iteration 61, loss = 0.01973701\n",
      "Iteration 62, loss = 0.01976728\n",
      "Iteration 63, loss = 0.01970530\n",
      "Iteration 64, loss = 0.01949180\n",
      "Iteration 65, loss = 0.01967239\n",
      "Iteration 66, loss = 0.01949324\n",
      "Iteration 67, loss = 0.01932467\n",
      "Iteration 68, loss = 0.01921865\n",
      "Iteration 69, loss = 0.01920271\n",
      "Iteration 70, loss = 0.01905747\n",
      "Iteration 71, loss = 0.01900959\n",
      "Iteration 72, loss = 0.01888040\n",
      "Iteration 73, loss = 0.01893589\n",
      "Iteration 74, loss = 0.01889109\n",
      "Iteration 75, loss = 0.01889983\n",
      "Iteration 76, loss = 0.01894082\n",
      "Iteration 77, loss = 0.01867001\n",
      "Iteration 78, loss = 0.01871563\n",
      "Iteration 79, loss = 0.01868641\n",
      "Iteration 80, loss = 0.01858140\n",
      "Iteration 81, loss = 0.01856099\n",
      "Iteration 82, loss = 0.01853385\n",
      "Iteration 83, loss = 0.01847158\n",
      "Iteration 84, loss = 0.01848165\n",
      "Iteration 85, loss = 0.01842779\n",
      "Iteration 86, loss = 0.01837321\n",
      "Iteration 87, loss = 0.01846771\n",
      "Iteration 88, loss = 0.01821239\n",
      "Iteration 89, loss = 0.01819866\n",
      "Iteration 90, loss = 0.01820282\n",
      "Iteration 91, loss = 0.01821423\n",
      "Iteration 92, loss = 0.01812003\n",
      "Iteration 93, loss = 0.01812842\n",
      "Iteration 94, loss = 0.01810136\n",
      "Iteration 95, loss = 0.01805052\n",
      "Iteration 96, loss = 0.01818271\n",
      "Iteration 97, loss = 0.01809514\n",
      "Iteration 98, loss = 0.01801719\n",
      "Iteration 99, loss = 0.01787673\n",
      "Iteration 100, loss = 0.01810501\n",
      "Iteration 101, loss = 0.01800891\n",
      "Iteration 102, loss = 0.01792996\n",
      "Iteration 103, loss = 0.01784934\n",
      "Iteration 104, loss = 0.01781494\n",
      "Iteration 105, loss = 0.01777876\n",
      "Iteration 106, loss = 0.01782980\n",
      "Iteration 107, loss = 0.01782822\n",
      "Iteration 108, loss = 0.01771559\n",
      "Iteration 109, loss = 0.01778664\n",
      "Iteration 110, loss = 0.01783950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:26<00:00,  5.36s/it]\n",
      "  0%|                                                                                                                                                                  | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.18359424\n",
      "Iteration 2, loss = 0.07685541\n",
      "Iteration 3, loss = 0.05809274\n",
      "Iteration 4, loss = 0.04837440\n",
      "Iteration 5, loss = 0.04296994\n",
      "Iteration 6, loss = 0.03907537\n",
      "Iteration 7, loss = 0.03627663\n",
      "Iteration 8, loss = 0.03396509\n",
      "Iteration 9, loss = 0.03210722\n",
      "Iteration 10, loss = 0.03084006\n",
      "Iteration 11, loss = 0.02968153\n",
      "Iteration 12, loss = 0.02880621\n",
      "Iteration 13, loss = 0.02804544\n",
      "Iteration 14, loss = 0.02734849\n",
      "Iteration 15, loss = 0.02667579\n",
      "Iteration 16, loss = 0.02613725\n",
      "Iteration 17, loss = 0.02557534\n",
      "Iteration 18, loss = 0.02514479\n",
      "Iteration 19, loss = 0.02467814\n",
      "Iteration 20, loss = 0.02426020\n",
      "Iteration 21, loss = 0.02373319\n",
      "Iteration 22, loss = 0.02335989\n",
      "Iteration 23, loss = 0.02297041\n",
      "Iteration 24, loss = 0.02251104\n",
      "Iteration 25, loss = 0.02207639\n",
      "Iteration 26, loss = 0.02165790\n",
      "Iteration 27, loss = 0.02140066\n",
      "Iteration 28, loss = 0.02115931\n",
      "Iteration 29, loss = 0.02097341\n",
      "Iteration 30, loss = 0.02068225\n",
      "Iteration 31, loss = 0.02041985\n",
      "Iteration 32, loss = 0.02019245\n",
      "Iteration 33, loss = 0.02005766\n",
      "Iteration 34, loss = 0.01991053\n",
      "Iteration 35, loss = 0.01966931\n",
      "Iteration 36, loss = 0.01953668\n",
      "Iteration 37, loss = 0.01951786\n",
      "Iteration 38, loss = 0.01930791\n",
      "Iteration 39, loss = 0.01914081\n",
      "Iteration 40, loss = 0.01906114\n",
      "Iteration 41, loss = 0.01908286\n",
      "Iteration 42, loss = 0.01888236\n",
      "Iteration 43, loss = 0.01879140\n",
      "Iteration 44, loss = 0.01870628\n",
      "Iteration 45, loss = 0.01871523\n",
      "Iteration 46, loss = 0.01857054\n",
      "Iteration 47, loss = 0.01847467\n",
      "Iteration 48, loss = 0.01839728\n",
      "Iteration 49, loss = 0.01833955\n",
      "Iteration 50, loss = 0.01830886\n",
      "Iteration 51, loss = 0.01820706\n",
      "Iteration 52, loss = 0.01817022\n",
      "Iteration 53, loss = 0.01810983\n",
      "Iteration 54, loss = 0.01817296\n",
      "Iteration 55, loss = 0.01799079\n",
      "Iteration 56, loss = 0.01794865\n",
      "Iteration 57, loss = 0.01789239\n",
      "Iteration 58, loss = 0.01783713\n",
      "Iteration 59, loss = 0.01779007\n",
      "Iteration 60, loss = 0.01771936\n",
      "Iteration 61, loss = 0.01784132\n",
      "Iteration 62, loss = 0.01774514\n",
      "Iteration 63, loss = 0.01763644\n",
      "Iteration 64, loss = 0.01761344\n",
      "Iteration 65, loss = 0.01759995\n",
      "Iteration 66, loss = 0.01760559\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██████████████████████████████▊                                                                                                                           | 1/5 [00:05<00:20,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22084712\n",
      "Iteration 2, loss = 0.08170748\n",
      "Iteration 3, loss = 0.06007010\n",
      "Iteration 4, loss = 0.04839805\n",
      "Iteration 5, loss = 0.04181562\n",
      "Iteration 6, loss = 0.03785613\n",
      "Iteration 7, loss = 0.03506367\n",
      "Iteration 8, loss = 0.03265752\n",
      "Iteration 9, loss = 0.03100365\n",
      "Iteration 10, loss = 0.02949689\n",
      "Iteration 11, loss = 0.02840162\n",
      "Iteration 12, loss = 0.02729563\n",
      "Iteration 13, loss = 0.02620469\n",
      "Iteration 14, loss = 0.02564053\n",
      "Iteration 15, loss = 0.02505350\n",
      "Iteration 16, loss = 0.02420343\n",
      "Iteration 17, loss = 0.02350540\n",
      "Iteration 18, loss = 0.02294075\n",
      "Iteration 19, loss = 0.02252409\n",
      "Iteration 20, loss = 0.02200571\n",
      "Iteration 21, loss = 0.02167995\n",
      "Iteration 22, loss = 0.02131672\n",
      "Iteration 23, loss = 0.02097547\n",
      "Iteration 24, loss = 0.02054387\n",
      "Iteration 25, loss = 0.02032690\n",
      "Iteration 26, loss = 0.01992967\n",
      "Iteration 27, loss = 0.01989283\n",
      "Iteration 28, loss = 0.01942692\n",
      "Iteration 29, loss = 0.01905425\n",
      "Iteration 30, loss = 0.01883625\n",
      "Iteration 31, loss = 0.01858553\n",
      "Iteration 32, loss = 0.01839948\n",
      "Iteration 33, loss = 0.01829000\n",
      "Iteration 34, loss = 0.01809497\n",
      "Iteration 35, loss = 0.01783337\n",
      "Iteration 36, loss = 0.01764715\n",
      "Iteration 37, loss = 0.01730966\n",
      "Iteration 38, loss = 0.01721828\n",
      "Iteration 39, loss = 0.01720064\n",
      "Iteration 40, loss = 0.01681957\n",
      "Iteration 41, loss = 0.01674223\n",
      "Iteration 42, loss = 0.01651151\n",
      "Iteration 43, loss = 0.01654044\n",
      "Iteration 44, loss = 0.01634662\n",
      "Iteration 45, loss = 0.01621450\n",
      "Iteration 46, loss = 0.01611155\n",
      "Iteration 47, loss = 0.01589508\n",
      "Iteration 48, loss = 0.01585738\n",
      "Iteration 49, loss = 0.01575640\n",
      "Iteration 50, loss = 0.01571810\n",
      "Iteration 51, loss = 0.01559244\n",
      "Iteration 52, loss = 0.01548324\n",
      "Iteration 53, loss = 0.01570477\n",
      "Iteration 54, loss = 0.01540293\n",
      "Iteration 55, loss = 0.01516616\n",
      "Iteration 56, loss = 0.01514436\n",
      "Iteration 57, loss = 0.01506355\n",
      "Iteration 58, loss = 0.01494238\n",
      "Iteration 59, loss = 0.01493563\n",
      "Iteration 60, loss = 0.01496792\n",
      "Iteration 61, loss = 0.01481762\n",
      "Iteration 62, loss = 0.01470770\n",
      "Iteration 63, loss = 0.01472273\n",
      "Iteration 64, loss = 0.01468769\n",
      "Iteration 65, loss = 0.01465302\n",
      "Iteration 66, loss = 0.01460275\n",
      "Iteration 67, loss = 0.01449726\n",
      "Iteration 68, loss = 0.01448287\n",
      "Iteration 69, loss = 0.01441430\n",
      "Iteration 70, loss = 0.01433155\n",
      "Iteration 71, loss = 0.01429487\n",
      "Iteration 72, loss = 0.01420656\n",
      "Iteration 73, loss = 0.01429973\n",
      "Iteration 74, loss = 0.01428737\n",
      "Iteration 75, loss = 0.01418385\n",
      "Iteration 76, loss = 0.01409511\n",
      "Iteration 77, loss = 0.01406348\n",
      "Iteration 78, loss = 0.01407246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████████████████████████████▌                                                                                            | 2/5 [00:10<00:15,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22153598\n",
      "Iteration 2, loss = 0.08374079\n",
      "Iteration 3, loss = 0.05804548\n",
      "Iteration 4, loss = 0.04699388\n",
      "Iteration 5, loss = 0.04023279\n",
      "Iteration 6, loss = 0.03605691\n",
      "Iteration 7, loss = 0.03324592\n",
      "Iteration 8, loss = 0.03134147\n",
      "Iteration 9, loss = 0.02965208\n",
      "Iteration 10, loss = 0.02848073\n",
      "Iteration 11, loss = 0.02750120\n",
      "Iteration 12, loss = 0.02680018\n",
      "Iteration 13, loss = 0.02604950\n",
      "Iteration 14, loss = 0.02539113\n",
      "Iteration 15, loss = 0.02491741\n",
      "Iteration 16, loss = 0.02418262\n",
      "Iteration 17, loss = 0.02361123\n",
      "Iteration 18, loss = 0.02305525\n",
      "Iteration 19, loss = 0.02274681\n",
      "Iteration 20, loss = 0.02202410\n",
      "Iteration 21, loss = 0.02160141\n",
      "Iteration 22, loss = 0.02112337\n",
      "Iteration 23, loss = 0.02063680\n",
      "Iteration 24, loss = 0.02029891\n",
      "Iteration 25, loss = 0.01982087\n",
      "Iteration 26, loss = 0.01959261\n",
      "Iteration 27, loss = 0.01934102\n",
      "Iteration 28, loss = 0.01901038\n",
      "Iteration 29, loss = 0.01872131\n",
      "Iteration 30, loss = 0.01861189\n",
      "Iteration 31, loss = 0.01839147\n",
      "Iteration 32, loss = 0.01809343\n",
      "Iteration 33, loss = 0.01808436\n",
      "Iteration 34, loss = 0.01783644\n",
      "Iteration 35, loss = 0.01741994\n",
      "Iteration 36, loss = 0.01733275\n",
      "Iteration 37, loss = 0.01725642\n",
      "Iteration 38, loss = 0.01697212\n",
      "Iteration 39, loss = 0.01694626\n",
      "Iteration 40, loss = 0.01673056\n",
      "Iteration 41, loss = 0.01663244\n",
      "Iteration 42, loss = 0.01654030\n",
      "Iteration 43, loss = 0.01646823\n",
      "Iteration 44, loss = 0.01627141\n",
      "Iteration 45, loss = 0.01603414\n",
      "Iteration 46, loss = 0.01596126\n",
      "Iteration 47, loss = 0.01588707\n",
      "Iteration 48, loss = 0.01573485\n",
      "Iteration 49, loss = 0.01560479\n",
      "Iteration 50, loss = 0.01547450\n",
      "Iteration 51, loss = 0.01538077\n",
      "Iteration 52, loss = 0.01546313\n",
      "Iteration 53, loss = 0.01521186\n",
      "Iteration 54, loss = 0.01523611\n",
      "Iteration 55, loss = 0.01526663\n",
      "Iteration 56, loss = 0.01498753\n",
      "Iteration 57, loss = 0.01499477\n",
      "Iteration 58, loss = 0.01496126\n",
      "Iteration 59, loss = 0.01483410\n",
      "Iteration 60, loss = 0.01478897\n",
      "Iteration 61, loss = 0.01471651\n",
      "Iteration 62, loss = 0.01470278\n",
      "Iteration 63, loss = 0.01464871\n",
      "Iteration 64, loss = 0.01459122\n",
      "Iteration 65, loss = 0.01445747\n",
      "Iteration 66, loss = 0.01449171\n",
      "Iteration 67, loss = 0.01439233\n",
      "Iteration 68, loss = 0.01430221\n",
      "Iteration 69, loss = 0.01420016\n",
      "Iteration 70, loss = 0.01426939\n",
      "Iteration 71, loss = 0.01414358\n",
      "Iteration 72, loss = 0.01407004\n",
      "Iteration 73, loss = 0.01417562\n",
      "Iteration 74, loss = 0.01393398\n",
      "Iteration 75, loss = 0.01398856\n",
      "Iteration 76, loss = 0.01392142\n",
      "Iteration 77, loss = 0.01388050\n",
      "Iteration 78, loss = 0.01375958\n",
      "Iteration 79, loss = 0.01376036\n",
      "Iteration 80, loss = 0.01367962\n",
      "Iteration 81, loss = 0.01368173\n",
      "Iteration 82, loss = 0.01362125\n",
      "Iteration 83, loss = 0.01351074\n",
      "Iteration 84, loss = 0.01354394\n",
      "Iteration 85, loss = 0.01362613\n",
      "Iteration 86, loss = 0.01338946\n",
      "Iteration 87, loss = 0.01352898\n",
      "Iteration 88, loss = 0.01335732\n",
      "Iteration 89, loss = 0.01339410\n",
      "Iteration 90, loss = 0.01319484\n",
      "Iteration 91, loss = 0.01317435\n",
      "Iteration 92, loss = 0.01320535\n",
      "Iteration 93, loss = 0.01320492\n",
      "Iteration 94, loss = 0.01326109\n",
      "Iteration 95, loss = 0.01314543\n",
      "Iteration 96, loss = 0.01299894\n",
      "Iteration 97, loss = 0.01308302\n",
      "Iteration 98, loss = 0.01307448\n",
      "Iteration 99, loss = 0.01287364\n",
      "Iteration 100, loss = 0.01295562\n",
      "Iteration 101, loss = 0.01286767\n",
      "Iteration 102, loss = 0.01294259\n",
      "Iteration 103, loss = 0.01289293\n",
      "Iteration 104, loss = 0.01279647\n",
      "Iteration 105, loss = 0.01283672\n",
      "Iteration 106, loss = 0.01279485\n",
      "Iteration 107, loss = 0.01276852\n",
      "Iteration 108, loss = 0.01268753\n",
      "Iteration 109, loss = 0.01264474\n",
      "Iteration 110, loss = 0.01261276\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 3/5 [00:16<00:11,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.23346398\n",
      "Iteration 2, loss = 0.10282189\n",
      "Iteration 3, loss = 0.07584789\n",
      "Iteration 4, loss = 0.06449273\n",
      "Iteration 5, loss = 0.05795407\n",
      "Iteration 6, loss = 0.05307055\n",
      "Iteration 7, loss = 0.04926326\n",
      "Iteration 8, loss = 0.04648654\n",
      "Iteration 9, loss = 0.04394339\n",
      "Iteration 10, loss = 0.04218372\n",
      "Iteration 11, loss = 0.04052135\n",
      "Iteration 12, loss = 0.03896159\n",
      "Iteration 13, loss = 0.03802698\n",
      "Iteration 14, loss = 0.03705023\n",
      "Iteration 15, loss = 0.03603457\n",
      "Iteration 16, loss = 0.03517423\n",
      "Iteration 17, loss = 0.03428490\n",
      "Iteration 18, loss = 0.03349329\n",
      "Iteration 19, loss = 0.03268712\n",
      "Iteration 20, loss = 0.03202706\n",
      "Iteration 21, loss = 0.03129437\n",
      "Iteration 22, loss = 0.03061615\n",
      "Iteration 23, loss = 0.03006912\n",
      "Iteration 24, loss = 0.02946501\n",
      "Iteration 25, loss = 0.02884642\n",
      "Iteration 26, loss = 0.02844944\n",
      "Iteration 27, loss = 0.02793803\n",
      "Iteration 28, loss = 0.02756423\n",
      "Iteration 29, loss = 0.02710669\n",
      "Iteration 30, loss = 0.02662618\n",
      "Iteration 31, loss = 0.02639785\n",
      "Iteration 32, loss = 0.02595234\n",
      "Iteration 33, loss = 0.02554152\n",
      "Iteration 34, loss = 0.02532849\n",
      "Iteration 35, loss = 0.02519356\n",
      "Iteration 36, loss = 0.02491148\n",
      "Iteration 37, loss = 0.02481557\n",
      "Iteration 38, loss = 0.02448628\n",
      "Iteration 39, loss = 0.02414093\n",
      "Iteration 40, loss = 0.02398306\n",
      "Iteration 41, loss = 0.02375880\n",
      "Iteration 42, loss = 0.02375321\n",
      "Iteration 43, loss = 0.02340040\n",
      "Iteration 44, loss = 0.02325613\n",
      "Iteration 45, loss = 0.02286969\n",
      "Iteration 46, loss = 0.02265241\n",
      "Iteration 47, loss = 0.02271039\n",
      "Iteration 48, loss = 0.02232665\n",
      "Iteration 49, loss = 0.02223191\n",
      "Iteration 50, loss = 0.02200432\n",
      "Iteration 51, loss = 0.02181016\n",
      "Iteration 52, loss = 0.02190934\n",
      "Iteration 53, loss = 0.02170151\n",
      "Iteration 54, loss = 0.02150538\n",
      "Iteration 55, loss = 0.02137014\n",
      "Iteration 56, loss = 0.02137946\n",
      "Iteration 57, loss = 0.02127950\n",
      "Iteration 58, loss = 0.02108191\n",
      "Iteration 59, loss = 0.02113838\n",
      "Iteration 60, loss = 0.02092553\n",
      "Iteration 61, loss = 0.02085811\n",
      "Iteration 62, loss = 0.02088219\n",
      "Iteration 63, loss = 0.02070610\n",
      "Iteration 64, loss = 0.02073828\n",
      "Iteration 65, loss = 0.02057035\n",
      "Iteration 66, loss = 0.02039117\n",
      "Iteration 67, loss = 0.02032607\n",
      "Iteration 68, loss = 0.02036016\n",
      "Iteration 69, loss = 0.02030836\n",
      "Iteration 70, loss = 0.02020331\n",
      "Iteration 71, loss = 0.02010025\n",
      "Iteration 72, loss = 0.02009246\n",
      "Iteration 73, loss = 0.02008156\n",
      "Iteration 74, loss = 0.02006906\n",
      "Iteration 75, loss = 0.02003887\n",
      "Iteration 76, loss = 0.01987693\n",
      "Iteration 77, loss = 0.01985527\n",
      "Iteration 78, loss = 0.01978690\n",
      "Iteration 79, loss = 0.01979162\n",
      "Iteration 80, loss = 0.01983419\n",
      "Iteration 81, loss = 0.01977687\n",
      "Iteration 82, loss = 0.01972156\n",
      "Iteration 83, loss = 0.01972420\n",
      "Iteration 84, loss = 0.01960395\n",
      "Iteration 85, loss = 0.01964941\n",
      "Iteration 86, loss = 0.01956410\n",
      "Iteration 87, loss = 0.01962934\n",
      "Iteration 88, loss = 0.01948546\n",
      "Iteration 89, loss = 0.01949747\n",
      "Iteration 90, loss = 0.01937762\n",
      "Iteration 91, loss = 0.01945696\n",
      "Iteration 92, loss = 0.01947848\n",
      "Iteration 93, loss = 0.01940156\n",
      "Iteration 94, loss = 0.01937963\n",
      "Iteration 95, loss = 0.01932379\n",
      "Iteration 96, loss = 0.01921056\n",
      "Iteration 97, loss = 0.01926589\n",
      "Iteration 98, loss = 0.01942839\n",
      "Iteration 99, loss = 0.01925563\n",
      "Iteration 100, loss = 0.01925257\n",
      "Iteration 101, loss = 0.01917200\n",
      "Iteration 102, loss = 0.01913557\n",
      "Iteration 103, loss = 0.01911433\n",
      "Iteration 104, loss = 0.01920416\n",
      "Iteration 105, loss = 0.01914787\n",
      "Iteration 106, loss = 0.01906636\n",
      "Iteration 107, loss = 0.01905174\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 4/5 [00:22<00:05,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.28706884\n",
      "Iteration 2, loss = 0.11470892\n",
      "Iteration 3, loss = 0.08148420\n",
      "Iteration 4, loss = 0.06765659\n",
      "Iteration 5, loss = 0.06007952\n",
      "Iteration 6, loss = 0.05464139\n",
      "Iteration 7, loss = 0.05037714\n",
      "Iteration 8, loss = 0.04729987\n",
      "Iteration 9, loss = 0.04455525\n",
      "Iteration 10, loss = 0.04258145\n",
      "Iteration 11, loss = 0.04087619\n",
      "Iteration 12, loss = 0.03950157\n",
      "Iteration 13, loss = 0.03832303\n",
      "Iteration 14, loss = 0.03732114\n",
      "Iteration 15, loss = 0.03637593\n",
      "Iteration 16, loss = 0.03549283\n",
      "Iteration 17, loss = 0.03466590\n",
      "Iteration 18, loss = 0.03390004\n",
      "Iteration 19, loss = 0.03312506\n",
      "Iteration 20, loss = 0.03239690\n",
      "Iteration 21, loss = 0.03157069\n",
      "Iteration 22, loss = 0.03085733\n",
      "Iteration 23, loss = 0.03012490\n",
      "Iteration 24, loss = 0.02937540\n",
      "Iteration 25, loss = 0.02875330\n",
      "Iteration 26, loss = 0.02822294\n",
      "Iteration 27, loss = 0.02756501\n",
      "Iteration 28, loss = 0.02729231\n",
      "Iteration 29, loss = 0.02661284\n",
      "Iteration 30, loss = 0.02614959\n",
      "Iteration 31, loss = 0.02565538\n",
      "Iteration 32, loss = 0.02521612\n",
      "Iteration 33, loss = 0.02502095\n",
      "Iteration 34, loss = 0.02450238\n",
      "Iteration 35, loss = 0.02414722\n",
      "Iteration 36, loss = 0.02384608\n",
      "Iteration 37, loss = 0.02351759\n",
      "Iteration 38, loss = 0.02326885\n",
      "Iteration 39, loss = 0.02298220\n",
      "Iteration 40, loss = 0.02290081\n",
      "Iteration 41, loss = 0.02229689\n",
      "Iteration 42, loss = 0.02215059\n",
      "Iteration 43, loss = 0.02183492\n",
      "Iteration 44, loss = 0.02158694\n",
      "Iteration 45, loss = 0.02136722\n",
      "Iteration 46, loss = 0.02135501\n",
      "Iteration 47, loss = 0.02111228\n",
      "Iteration 48, loss = 0.02073069\n",
      "Iteration 49, loss = 0.02043216\n",
      "Iteration 50, loss = 0.02024095\n",
      "Iteration 51, loss = 0.01999777\n",
      "Iteration 52, loss = 0.01989745\n",
      "Iteration 53, loss = 0.01960369\n",
      "Iteration 54, loss = 0.01964513\n",
      "Iteration 55, loss = 0.01921505\n",
      "Iteration 56, loss = 0.01906535\n",
      "Iteration 57, loss = 0.01900308\n",
      "Iteration 58, loss = 0.01860540\n",
      "Iteration 59, loss = 0.01858846\n",
      "Iteration 60, loss = 0.01832451\n",
      "Iteration 61, loss = 0.01811190\n",
      "Iteration 62, loss = 0.01795097\n",
      "Iteration 63, loss = 0.01777683\n",
      "Iteration 64, loss = 0.01765690\n",
      "Iteration 65, loss = 0.01762825\n",
      "Iteration 66, loss = 0.01746725\n",
      "Iteration 67, loss = 0.01730664\n",
      "Iteration 68, loss = 0.01728357\n",
      "Iteration 69, loss = 0.01719034\n",
      "Iteration 70, loss = 0.01708590\n",
      "Iteration 71, loss = 0.01687423\n",
      "Iteration 72, loss = 0.01676660\n",
      "Iteration 73, loss = 0.01687210\n",
      "Iteration 74, loss = 0.01666651\n",
      "Iteration 75, loss = 0.01671533\n",
      "Iteration 76, loss = 0.01662474\n",
      "Iteration 77, loss = 0.01648328\n",
      "Iteration 78, loss = 0.01632452\n",
      "Iteration 79, loss = 0.01630323\n",
      "Iteration 80, loss = 0.01625819\n",
      "Iteration 81, loss = 0.01617587\n",
      "Iteration 82, loss = 0.01608734\n",
      "Iteration 83, loss = 0.01630423\n",
      "Iteration 84, loss = 0.01612213\n",
      "Iteration 85, loss = 0.01592622\n",
      "Iteration 86, loss = 0.01592816\n",
      "Iteration 87, loss = 0.01591737\n",
      "Iteration 88, loss = 0.01600401\n",
      "Iteration 89, loss = 0.01579550\n",
      "Iteration 90, loss = 0.01580512\n",
      "Iteration 91, loss = 0.01575421\n",
      "Iteration 92, loss = 0.01568247\n",
      "Iteration 93, loss = 0.01570343\n",
      "Iteration 94, loss = 0.01556884\n",
      "Iteration 95, loss = 0.01563160\n",
      "Iteration 96, loss = 0.01571881\n",
      "Iteration 97, loss = 0.01559663\n",
      "Iteration 98, loss = 0.01550525\n",
      "Iteration 99, loss = 0.01543514\n",
      "Iteration 100, loss = 0.01544387\n",
      "Iteration 101, loss = 0.01542378\n",
      "Iteration 102, loss = 0.01541849\n",
      "Iteration 103, loss = 0.01537093\n",
      "Iteration 104, loss = 0.01532566\n",
      "Iteration 105, loss = 0.01547067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:27<00:00,  5.44s/it]\n"
     ]
    }
   ],
   "source": [
    "enc_measures = index_series(\n",
    "    train_data.values,\n",
    "    window=12 * 3,\n",
    "    period=12,\n",
    "    model=lambda:Scaled(1.0, AutoencKnnWrapper(reg, KNeighborsRegressor(n_neighbors=50))),\n",
    "    step=6,\n",
    "    noise_iters=3,\n",
    "    noise_weight=1e-3,\n",
    "    sample=0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3621521373631788,\n",
       " 0.34097475092624124,\n",
       " {'p_value': 0.1682653919042325,\n",
       "  'mu': 0.021177386436937548,\n",
       "  'sigma': 0.09841888997030773,\n",
       "  'win_rate': 0.6521739130434783})"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(knn_measures[0]), np.mean(zero_measures[0]), z_test(knn_measures[0], zero_measures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f801ab1bb00>]"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.sort(knn_measures[0]), 'blue')\n",
    "plt.plot(np.sort(zero_measures[0]), 'orange')\n",
    "plt.plot(np.sort(enc_measures[0]), 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.624751242230631,\n",
       " 1.6973818018010343,\n",
       " {'p_value': 0.15601606042598445,\n",
       "  'mu': -0.07263055957040336,\n",
       "  'sigma': 0.3278323274794252,\n",
       "  'win_rate': 0.38067632850241545})"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(knn_measures[1]), np.mean(zero_measures[1]), z_test(knn_measures[1], zero_measures[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3793838449769264,\n",
       " 0.34097475092624124,\n",
       " {'p_value': 0.12108405474827573,\n",
       "  'mu': 0.038409094050685086,\n",
       "  'sigma': 0.15864529915792094,\n",
       "  'win_rate': 0.5217391304347826})"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(enc_measures[0]), np.mean(zero_measures[0]), z_test(enc_measures[0], zero_measures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.598524526316335,\n",
       " 1.6973818018010343,\n",
       " {'p_value': 0.2950455827771611,\n",
       "  'mu': -0.09885727548469926,\n",
       "  'sigma': 0.6045125505808853,\n",
       "  'win_rate': 0.4975845410628019})"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(enc_measures[1]), np.mean(zero_measures[1]), z_test(enc_measures[1], zero_measures[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.598524526316335,\n",
       " 1.624751242230631,\n",
       " {'p_value': 0.5990975967436475,\n",
       "  'mu': -0.02622671591429589,\n",
       "  'sigma': 0.31944711661218467,\n",
       "  'win_rate': 0.578743961352657})"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(enc_measures[1]), np.mean(knn_measures[1]), z_test(enc_measures[1], knn_measures[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3793838449769264,\n",
       " 0.3621521373631788,\n",
       " {'p_value': 0.42105169453149205,\n",
       "  'mu': 0.01723170761374753,\n",
       "  'sigma': 0.1371327845366476,\n",
       "  'win_rate': 0.4492753623188406})"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(enc_measures[0]), np.mean(knn_measures[0]), z_test(enc_measures[0], knn_measures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = MLPRegressor(hidden_layer_sizes = (windowm, 16, 8, 16, windowm), \n",
    "#                    activation = 'tanh', \n",
    "#                    solver = 'adam',\n",
    "# #                    learning_rate_init = 0.0001, \n",
    "# #                    max_iter = 20, \n",
    "# #                    tol = 0.0000001, \n",
    "#                    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156, 69)"
      ]
     },
     "execution_count": 2645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn:  1.5445594509414997 (0.37191490318074705, [0.2674416612166981, 0.29322322485038255, 0.41395979153041784, 0.40578971850697076, 0.20106105902609084, 0.2582392774903457, 0.20046341391457934, 0.20000202626849095, 0.2187227492335535, 0.19999999999999996, 0.19999999999999996, 0.20000000000000306, 0.37662327746227686, 0.4065722591008816, 0.2662174474308223, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996, 0.3098394254135928, 0.2002307989109331, 0.3657653386989981, 0.20477182166466323, 0.2000000000011648, 0.20000000000086748, 0.19999999999999996, 0.2000098598797544, 0.20000000644424354, 0.20024037926109073, 0.20000000005216623, 0.20000293186555207, 0.20000021355677644, 0.2915202616692658, 0.28575009284869557, 0.2003206908037336, 0.20000639524834307, 0.20000635671766864, 0.20090779378570733, 0.19999999999999996, 0.5756888979582802, 0.6620031256439582, 0.9572522627760223, 0.6790192815439942, 0.42873913734047586, 0.913759659282092, 0.5286361417952872, 0.6001455337444197, 0.47276265726406064, 0.887078861074692, 0.6294253266704355, 0.20474473827626327, 0.4964872355844321, 0.9331391713370931, 0.7009138729014344, 0.4687773491892866, 0.2000000000010751, 0.36000206751563746, 0.2025668396481184, 0.2000057312514909, 0.8966086163365079, 0.5952036322563341, 0.2003645472306721, 0.5451842014137374, 0.5466462321588779, 0.20001715222064909, 0.3433481471338313, 0.866492145234804, 0.7938854626190126, 0.20554201921383863])\n",
      "zero:  1.6253887423863334 (0.33289125182923773, [0.2675590185086647, 0.28260183370681813, 0.46915238829368455, 0.45712182783601985, 0.2010438464216331, 0.2709280060651984, 0.20032409597557232, 0.20000240308795636, 0.22409219553585147, 0.19999999999999996, 0.19999999999999996, 0.20000000000000218, 0.3691104285316731, 0.39859377452511335, 0.26634180555636355, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996, 0.19999999999999996, 0.35509002510621945, 0.20061781057883454, 0.3821171805292862, 0.20614741511624035, 0.2000000000104616, 0.2000000000240425, 0.19999999999999996, 0.20000266606767392, 0.20000003027281998, 0.20002880054941619, 0.20000000035096188, 0.20000179422811382, 0.20000076157379798, 0.2147415732542799, 0.3128974225080894, 0.2002638043652607, 0.20000527093876652, 0.20000000002014673, 0.20000368890336206, 0.19999999999999996, 0.20942070813979852, 0.5457696809453603, 0.8430299412457408, 0.5509564056417937, 0.39530900131161983, 0.8311880097473104, 0.5815827533497833, 0.5177598860568746, 0.23832558013518534, 0.7719386218436477, 0.673834952292331, 0.20252625147931558, 0.21308909277941712, 0.791238024636753, 0.34153831849836447, 0.5065051641323675, 0.20000000000342966, 0.40176130873700977, 0.2011129857181415, 0.20003063391958742, 0.5559658654213533, 0.8251288386371541, 0.20031120549515413, 0.44169249467682725, 0.48793330273993596, 0.20000912377631086, 0.285082615481308, 0.6982308738813734, 0.36817690206349907, 0.2112579649883315])\n",
      "log z-test:  {'p_value': 0.006014958469610724, 'mu': 0.20414578353484855, 'sigma': 0.4641086034177921, 'win_rate': 0.6086956521739131}\n",
      "Iteration 1, loss = 0.15575400\n",
      "Iteration 2, loss = 0.06842674\n",
      "Iteration 3, loss = 0.05116565\n",
      "Iteration 4, loss = 0.04169489\n",
      "Iteration 5, loss = 0.03692063\n",
      "Iteration 6, loss = 0.03393693\n",
      "Iteration 7, loss = 0.03161106\n",
      "Iteration 8, loss = 0.02993366\n",
      "Iteration 9, loss = 0.02868107\n",
      "Iteration 10, loss = 0.02760991\n",
      "Iteration 11, loss = 0.02668006\n",
      "Iteration 12, loss = 0.02596375\n",
      "Iteration 13, loss = 0.02516502\n",
      "Iteration 14, loss = 0.02437250\n",
      "Iteration 15, loss = 0.02388473\n",
      "Iteration 16, loss = 0.02331146\n",
      "Iteration 17, loss = 0.02287846\n",
      "Iteration 18, loss = 0.02245234\n",
      "Iteration 19, loss = 0.02211890\n",
      "Iteration 20, loss = 0.02182063\n",
      "Iteration 21, loss = 0.02164137\n",
      "Iteration 22, loss = 0.02133066\n",
      "Iteration 23, loss = 0.02111435\n",
      "Iteration 24, loss = 0.02092429\n",
      "Iteration 25, loss = 0.02078213\n",
      "Iteration 26, loss = 0.02060860\n",
      "Iteration 27, loss = 0.02044057\n",
      "Iteration 28, loss = 0.02028239\n",
      "Iteration 29, loss = 0.02012053\n",
      "Iteration 30, loss = 0.01999942\n",
      "Iteration 31, loss = 0.01983931\n",
      "Iteration 32, loss = 0.01967157\n",
      "Iteration 33, loss = 0.01938044\n",
      "Iteration 34, loss = 0.01922813\n",
      "Iteration 35, loss = 0.01903877\n",
      "Iteration 36, loss = 0.01892391\n",
      "Iteration 37, loss = 0.01884398\n",
      "Iteration 38, loss = 0.01870736\n",
      "Iteration 39, loss = 0.01856404\n",
      "Iteration 40, loss = 0.01844121\n",
      "Iteration 41, loss = 0.01842089\n",
      "Iteration 42, loss = 0.01829219\n",
      "Iteration 43, loss = 0.01818262\n",
      "Iteration 44, loss = 0.01814742\n",
      "Iteration 45, loss = 0.01802894\n",
      "Iteration 46, loss = 0.01791389\n",
      "Iteration 47, loss = 0.01792916\n",
      "Iteration 48, loss = 0.01783706\n",
      "Iteration 49, loss = 0.01768803\n",
      "Iteration 50, loss = 0.01773538\n",
      "Iteration 51, loss = 0.01763580\n",
      "Iteration 52, loss = 0.01760983\n",
      "Iteration 53, loss = 0.01746907\n",
      "Iteration 54, loss = 0.01737545\n",
      "Iteration 55, loss = 0.01728012\n",
      "Iteration 56, loss = 0.01721894\n",
      "Iteration 57, loss = 0.01721669\n",
      "Iteration 58, loss = 0.01723921\n",
      "Iteration 59, loss = 0.01714242\n",
      "Iteration 60, loss = 0.01708091\n",
      "Iteration 61, loss = 0.01702407\n",
      "Iteration 62, loss = 0.01700024\n",
      "Iteration 63, loss = 0.01706944\n",
      "Iteration 64, loss = 0.01690764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 39\n",
    "\n",
    "offset = 15\n",
    "index = SearchSequenceIndex(window=12 * 3, period=12, offset=offset)\n",
    "\n",
    "pref, full = index.get_train_part_and_full(train_data.values)\n",
    "index.build(pref)\n",
    "\n",
    "orig = train_data.values[:, T]\n",
    "window_data = orig[:-offset]\n",
    "\n",
    "index.fit(Scaled(0.5, KNeighborsRegressor(n_neighbors=50)))\n",
    "r1 = index.predict_for_seq(window_data, offset)\n",
    "knn_res = index.measure_on_all_train_data(full.T, offset=offset)\n",
    "knn_scores = wmsfe(knn_res)\n",
    "print('knn: ', knn_scores[0], leaderboard([[x] for x in knn_res]))\n",
    "\n",
    "index.fit(LinearRegression())\n",
    "r2 = index.predict_for_seq(window_data, offset)\n",
    "\n",
    "index.fit(ZeroModel())\n",
    "r3 = index.predict_for_seq(window_data, offset)\n",
    "zero_res = index.measure_on_all_train_data(full.T, offset=offset)\n",
    "zero_scores = wmsfe(zero_res)\n",
    "\n",
    "print('zero: ', zero_scores[0], leaderboard([[x] for x in zero_res]))\n",
    "\n",
    "print('log z-test: ', log_z_test(knn_scores[1], zero_scores[1]))\n",
    "\n",
    "index.fit(AutoencKnnWrapper(reg, KNeighborsRegressor(n_neighbors=50)))\n",
    "r4 = index.predict_for_seq(window_data, offset)\n",
    "res = index.measure_on_all_train_data(full.T, offset=offset)\n",
    "scores = wmsfe(res)\n",
    "\n",
    "plt.plot(r1, 'red')\n",
    "plt.plot(r2, 'green')\n",
    "plt.plot(r3, 'orange')\n",
    "plt.plot(r4, 'purple')\n",
    "plt.plot(train_data.values[:, T], 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for i in range(14):\n",
    "    test_data.append(pd.read_excel(\"../../data/unzipped/Test_example{}.xlsx\".format(i+1), index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for i in range(14):\n",
    "    for f in np.arange(test_data[i].shape[1]):\n",
    "        seq = test_data[i].values[:, f]\n",
    "        seq = seq[np.where(seq != \"Forecast\")]\n",
    "        if len(seq) >= 12 * 5:\n",
    "            X.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 58)"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([len(x) for x in X if len(x) >= 12 * 5]), len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.11553871\n",
      "Iteration 2, loss = 0.05028768\n",
      "Iteration 3, loss = 0.03881310\n",
      "Iteration 4, loss = 0.03312687\n",
      "Iteration 5, loss = 0.03001217\n",
      "Iteration 6, loss = 0.02812924\n",
      "Iteration 7, loss = 0.02679238\n",
      "Iteration 8, loss = 0.02567155\n",
      "Iteration 9, loss = 0.02480812\n",
      "Iteration 10, loss = 0.02397167\n",
      "Iteration 11, loss = 0.02320318\n",
      "Iteration 12, loss = 0.02247370\n",
      "Iteration 13, loss = 0.02204387\n",
      "Iteration 14, loss = 0.02148675\n",
      "Iteration 15, loss = 0.02102921\n",
      "Iteration 16, loss = 0.02058375\n",
      "Iteration 17, loss = 0.02019548\n",
      "Iteration 18, loss = 0.01994778\n",
      "Iteration 19, loss = 0.01975606\n",
      "Iteration 20, loss = 0.01955177\n",
      "Iteration 21, loss = 0.01935262\n",
      "Iteration 22, loss = 0.01921650\n",
      "Iteration 23, loss = 0.01911270\n",
      "Iteration 24, loss = 0.01900069\n",
      "Iteration 25, loss = 0.01890512\n",
      "Iteration 26, loss = 0.01880410\n",
      "Iteration 27, loss = 0.01867410\n",
      "Iteration 28, loss = 0.01865139\n",
      "Iteration 29, loss = 0.01857854\n",
      "Iteration 30, loss = 0.01856639\n",
      "Iteration 31, loss = 0.01843961\n",
      "Iteration 32, loss = 0.01845620\n",
      "Iteration 33, loss = 0.01831951\n",
      "Iteration 34, loss = 0.01827052\n",
      "Iteration 35, loss = 0.01818709\n",
      "Iteration 36, loss = 0.01817546\n",
      "Iteration 37, loss = 0.01814123\n",
      "Iteration 38, loss = 0.01812853\n",
      "Iteration 39, loss = 0.01799066\n",
      "Iteration 40, loss = 0.01801750\n",
      "Iteration 41, loss = 0.01783829\n",
      "Iteration 42, loss = 0.01784823\n",
      "Iteration 43, loss = 0.01765122\n",
      "Iteration 44, loss = 0.01769361\n",
      "Iteration 45, loss = 0.01755322\n",
      "Iteration 46, loss = 0.01749114\n",
      "Iteration 47, loss = 0.01742308\n",
      "Iteration 48, loss = 0.01737465\n",
      "Iteration 49, loss = 0.01729321\n",
      "Iteration 50, loss = 0.01732393\n",
      "Iteration 51, loss = 0.01718202\n",
      "Iteration 52, loss = 0.01714202\n",
      "Iteration 53, loss = 0.01711895\n",
      "Iteration 54, loss = 0.01715438\n",
      "Iteration 55, loss = 0.01707452\n",
      "Iteration 56, loss = 0.01697085\n",
      "Iteration 57, loss = 0.01697867\n",
      "Iteration 58, loss = 0.01697881\n",
      "Iteration 59, loss = 0.01692041\n",
      "Iteration 60, loss = 0.01693179\n",
      "Iteration 61, loss = 0.01691047\n",
      "Iteration 62, loss = 0.01686523\n",
      "Iteration 63, loss = 0.01690379\n",
      "Iteration 64, loss = 0.01684447\n",
      "Iteration 65, loss = 0.01684448\n",
      "Iteration 66, loss = 0.01675751\n",
      "Iteration 67, loss = 0.01679584\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "knn:  0.18657775705046728\n",
      "zero:  0.2180429271995853\n",
      "z-test:  {'p_value': 0.02312626231098711, 'mu': -0.03146517014911803, 'sigma': 0.08651268528917724, 'win_rate': 0.3793103448275862}\n"
     ]
    }
   ],
   "source": [
    "index = SearchSequenceIndex(window=12 * 3, period=12, offset=15)\n",
    "index.build(train_data.values)\n",
    "\n",
    "index.fit(AutoencKnnWrapper(reg, KNeighborsRegressor(n_neighbors=50)))\n",
    "knn_res = index.measure_on_all_train_data(X, offset=15)\n",
    "knn_scores = wmsfe(knn_res)\n",
    "print('knn: ', np.mean(knn_scores[0]))#, leaderboard([[x] for x in knn_res]))\n",
    "\n",
    "index.fit(ZeroModel())\n",
    "zero_res = index.measure_on_all_train_data(X, offset=15)\n",
    "zero_scores = wmsfe(zero_res)\n",
    "\n",
    "print('zero: ', np.mean(zero_scores[0]))#, leaderboard([[x] for x in zero_res]))\n",
    "\n",
    "print('z-test: ', z_test(knn_scores[1], zero_scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "quartal_data = pd.read_excel(\"../../data/unzipped/Train.xlsx\", skiprows=range(1,2), index_col=0,sheet_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ВВП, млрд</th>\n",
       "      <th>Реальный ВВП</th>\n",
       "      <th>ВВП A, млрд.руб.</th>\n",
       "      <th>ВВП B, млрд.руб.</th>\n",
       "      <th>ВВП C, млрд.руб.</th>\n",
       "      <th>ВВП D, млрд.руб.</th>\n",
       "      <th>ВВП E, млрд.руб.</th>\n",
       "      <th>ВВП F, млрд.руб.</th>\n",
       "      <th>ВВП G, млрд.руб.</th>\n",
       "      <th>ВВП H, млрд.руб.</th>\n",
       "      <th>...</th>\n",
       "      <th>Реальный ВВП K</th>\n",
       "      <th>Реальный ВВП L</th>\n",
       "      <th>Реальный ВВП M</th>\n",
       "      <th>Реальный ВВП N</th>\n",
       "      <th>Реальный ВВП O</th>\n",
       "      <th>Реальные налоги на продукты</th>\n",
       "      <th>Реальные субсидии на продукты</th>\n",
       "      <th>Инвестиции, собственные средства, млрд.руб.</th>\n",
       "      <th>Инвестиции, привлеченные средства, млрд.руб.</th>\n",
       "      <th>Инвестиции, бюджетные средства, млрд.руб.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003m03</th>\n",
       "      <td>8.041233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.273602</td>\n",
       "      <td>2.633625</td>\n",
       "      <td>5.134760</td>\n",
       "      <td>5.926435</td>\n",
       "      <td>4.786542</td>\n",
       "      <td>4.884884</td>\n",
       "      <td>6.374866</td>\n",
       "      <td>2.944192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.872905</td>\n",
       "      <td>4.911183</td>\n",
       "      <td>3.832980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003m06</th>\n",
       "      <td>8.115856</td>\n",
       "      <td>0.067384</td>\n",
       "      <td>4.520113</td>\n",
       "      <td>2.511972</td>\n",
       "      <td>5.133081</td>\n",
       "      <td>6.073957</td>\n",
       "      <td>4.461686</td>\n",
       "      <td>5.208554</td>\n",
       "      <td>6.393228</td>\n",
       "      <td>3.151690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032777</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>-0.017488</td>\n",
       "      <td>0.016888</td>\n",
       "      <td>0.049110</td>\n",
       "      <td>0.047356</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>5.249127</td>\n",
       "      <td>5.455748</td>\n",
       "      <td>4.391977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003m09</th>\n",
       "      <td>8.255223</td>\n",
       "      <td>0.119733</td>\n",
       "      <td>5.690798</td>\n",
       "      <td>2.816459</td>\n",
       "      <td>5.328411</td>\n",
       "      <td>6.158050</td>\n",
       "      <td>4.357089</td>\n",
       "      <td>5.476453</td>\n",
       "      <td>6.477670</td>\n",
       "      <td>3.245579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042272</td>\n",
       "      <td>0.060595</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>0.037533</td>\n",
       "      <td>0.096204</td>\n",
       "      <td>0.063358</td>\n",
       "      <td>0.211227</td>\n",
       "      <td>5.367843</td>\n",
       "      <td>5.540086</td>\n",
       "      <td>4.513055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003m12</th>\n",
       "      <td>8.265189</td>\n",
       "      <td>0.171204</td>\n",
       "      <td>4.979387</td>\n",
       "      <td>2.691399</td>\n",
       "      <td>5.296727</td>\n",
       "      <td>6.212625</td>\n",
       "      <td>4.766878</td>\n",
       "      <td>5.468805</td>\n",
       "      <td>6.588304</td>\n",
       "      <td>3.254391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130738</td>\n",
       "      <td>0.090004</td>\n",
       "      <td>0.043583</td>\n",
       "      <td>0.019568</td>\n",
       "      <td>0.228176</td>\n",
       "      <td>0.100861</td>\n",
       "      <td>0.024644</td>\n",
       "      <td>5.668501</td>\n",
       "      <td>5.927459</td>\n",
       "      <td>4.940213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004m03</th>\n",
       "      <td>8.250766</td>\n",
       "      <td>0.069902</td>\n",
       "      <td>4.356881</td>\n",
       "      <td>2.822974</td>\n",
       "      <td>5.633453</td>\n",
       "      <td>6.213559</td>\n",
       "      <td>5.063559</td>\n",
       "      <td>5.077852</td>\n",
       "      <td>6.491535</td>\n",
       "      <td>3.362732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087127</td>\n",
       "      <td>0.037359</td>\n",
       "      <td>-0.001314</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>0.069604</td>\n",
       "      <td>0.096598</td>\n",
       "      <td>-0.046582</td>\n",
       "      <td>5.017942</td>\n",
       "      <td>5.226284</td>\n",
       "      <td>3.910021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ВВП, млрд  Реальный ВВП  ВВП A, млрд.руб.  ВВП B, млрд.руб.  \\\n",
       "2003m03   8.041233      0.000000          4.273602          2.633625   \n",
       "2003m06   8.115856      0.067384          4.520113          2.511972   \n",
       "2003m09   8.255223      0.119733          5.690798          2.816459   \n",
       "2003m12   8.265189      0.171204          4.979387          2.691399   \n",
       "2004m03   8.250766      0.069902          4.356881          2.822974   \n",
       "\n",
       "         ВВП C, млрд.руб.  ВВП D, млрд.руб.  ВВП E, млрд.руб.  \\\n",
       "2003m03          5.134760          5.926435          4.786542   \n",
       "2003m06          5.133081          6.073957          4.461686   \n",
       "2003m09          5.328411          6.158050          4.357089   \n",
       "2003m12          5.296727          6.212625          4.766878   \n",
       "2004m03          5.633453          6.213559          5.063559   \n",
       "\n",
       "         ВВП F, млрд.руб.  ВВП G, млрд.руб.  ВВП H, млрд.руб.  ...  \\\n",
       "2003m03          4.884884          6.374866          2.944192  ...   \n",
       "2003m06          5.208554          6.393228          3.151690  ...   \n",
       "2003m09          5.476453          6.477670          3.245579  ...   \n",
       "2003m12          5.468805          6.588304          3.254391  ...   \n",
       "2004m03          5.077852          6.491535          3.362732  ...   \n",
       "\n",
       "         Реальный ВВП K  Реальный ВВП L  Реальный ВВП M  Реальный ВВП N  \\\n",
       "2003m03        0.000000        0.000000        0.000000        0.000000   \n",
       "2003m06        0.032777        0.005775       -0.017488        0.016888   \n",
       "2003m09        0.042272        0.060595        0.019040        0.037533   \n",
       "2003m12        0.130738        0.090004        0.043583        0.019568   \n",
       "2004m03       -0.087127        0.037359       -0.001314        0.011056   \n",
       "\n",
       "         Реальный ВВП O  Реальные налоги на продукты  \\\n",
       "2003m03        0.000000                     0.000000   \n",
       "2003m06        0.049110                     0.047356   \n",
       "2003m09        0.096204                     0.063358   \n",
       "2003m12        0.228176                     0.100861   \n",
       "2004m03        0.069604                     0.096598   \n",
       "\n",
       "         Реальные субсидии на продукты  \\\n",
       "2003m03                       0.000000   \n",
       "2003m06                       0.032027   \n",
       "2003m09                       0.211227   \n",
       "2003m12                       0.024644   \n",
       "2004m03                      -0.046582   \n",
       "\n",
       "         Инвестиции, собственные средства, млрд.руб.  \\\n",
       "2003m03                                     4.872905   \n",
       "2003m06                                     5.249127   \n",
       "2003m09                                     5.367843   \n",
       "2003m12                                     5.668501   \n",
       "2004m03                                     5.017942   \n",
       "\n",
       "         Инвестиции, привлеченные средства, млрд.руб.  \\\n",
       "2003m03                                      4.911183   \n",
       "2003m06                                      5.455748   \n",
       "2003m09                                      5.540086   \n",
       "2003m12                                      5.927459   \n",
       "2004m03                                      5.226284   \n",
       "\n",
       "         Инвестиции, бюджетные средства, млрд.руб.  \n",
       "2003m03                                   3.832980  \n",
       "2003m06                                   4.391977  \n",
       "2003m09                                   4.513055  \n",
       "2003m12                                   4.940213  \n",
       "2004m03                                   3.910021  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quartal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 39)"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quartal_data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowq = 4  * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.17763052\n",
      "Iteration 2, loss = 0.05730871\n",
      "Iteration 3, loss = 0.03987083\n",
      "Iteration 4, loss = 0.03122359\n",
      "Iteration 5, loss = 0.02606020\n",
      "Iteration 6, loss = 0.02236719\n",
      "Iteration 7, loss = 0.01934756\n",
      "Iteration 8, loss = 0.01675167\n",
      "Iteration 9, loss = 0.01447772\n",
      "Iteration 10, loss = 0.01267143\n",
      "Iteration 11, loss = 0.01145221\n",
      "Iteration 12, loss = 0.01032097\n",
      "Iteration 13, loss = 0.00934638\n",
      "Iteration 14, loss = 0.00848448\n",
      "Iteration 15, loss = 0.00790324\n",
      "Iteration 16, loss = 0.00747203\n",
      "Iteration 17, loss = 0.00682574\n",
      "Iteration 18, loss = 0.00651482\n",
      "Iteration 19, loss = 0.00613609\n",
      "Iteration 20, loss = 0.00578451\n",
      "Iteration 21, loss = 0.00544661\n",
      "Iteration 22, loss = 0.00528916\n",
      "Iteration 23, loss = 0.00488579\n",
      "Iteration 24, loss = 0.00463711\n",
      "Iteration 25, loss = 0.00460749\n",
      "Iteration 26, loss = 0.00436320\n",
      "Iteration 27, loss = 0.00423243\n",
      "Iteration 28, loss = 0.00400708\n",
      "Iteration 29, loss = 0.00391018\n",
      "Iteration 30, loss = 0.00373296\n",
      "Iteration 31, loss = 0.00359876\n",
      "Iteration 32, loss = 0.00352903\n",
      "Iteration 33, loss = 0.00352572\n",
      "Iteration 34, loss = 0.00329296\n",
      "Iteration 35, loss = 0.00327083\n",
      "Iteration 36, loss = 0.00309472\n",
      "Iteration 37, loss = 0.00299539\n",
      "Iteration 38, loss = 0.00302562\n",
      "Iteration 39, loss = 0.00283464\n",
      "Iteration 40, loss = 0.00269671\n",
      "Iteration 41, loss = 0.00258945\n",
      "Iteration 42, loss = 0.00246882\n",
      "Iteration 43, loss = 0.00244793\n",
      "Iteration 44, loss = 0.00226485\n",
      "Iteration 45, loss = 0.00215633\n",
      "Iteration 46, loss = 0.00206523\n",
      "Iteration 47, loss = 0.00187119\n",
      "Iteration 48, loss = 0.00182596\n",
      "Iteration 49, loss = 0.00173393\n",
      "Iteration 50, loss = 0.00168578\n",
      "Iteration 51, loss = 0.00166153\n",
      "Iteration 52, loss = 0.00159303\n",
      "Iteration 53, loss = 0.00156256\n",
      "Iteration 54, loss = 0.00151931\n",
      "Iteration 55, loss = 0.00153153\n",
      "Iteration 56, loss = 0.00142016\n",
      "Iteration 57, loss = 0.00146429\n",
      "Iteration 58, loss = 0.00141265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "index = SearchSequenceIndex(window=windowq, period=4, offset=5)\n",
    "index.build(quartal_data.values)\n",
    "\n",
    "T = 30\n",
    "offset = 5\n",
    "orig = quartal_data.values[:, T]\n",
    "window_data = orig[:-offset]\n",
    "\n",
    "index.fit(AutoencKnnWrapper(reg, KNeighborsRegressor(n_neighbors=50)))\n",
    "r1 = index.predict_for_seq(window_data, offset)\n",
    "knn_res = index.measure_on_all_train_data(quartal_data.values, offset=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 29.39it/s]\n",
      "  0%|                                                                                                                                                                  | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.28816403\n",
      "Iteration 2, loss = 0.08787475\n",
      "Iteration 3, loss = 0.05661842\n",
      "Iteration 4, loss = 0.04721693\n",
      "Iteration 5, loss = 0.04186564\n",
      "Iteration 6, loss = 0.03750921\n",
      "Iteration 7, loss = 0.03378626\n",
      "Iteration 8, loss = 0.03038337\n",
      "Iteration 9, loss = 0.02731187\n",
      "Iteration 10, loss = 0.02483532\n",
      "Iteration 11, loss = 0.02261112\n",
      "Iteration 12, loss = 0.02081445\n",
      "Iteration 13, loss = 0.01947926\n",
      "Iteration 14, loss = 0.01820696\n",
      "Iteration 15, loss = 0.01707598\n",
      "Iteration 16, loss = 0.01604511\n",
      "Iteration 17, loss = 0.01503611\n",
      "Iteration 18, loss = 0.01421167\n",
      "Iteration 19, loss = 0.01340898\n",
      "Iteration 20, loss = 0.01258717\n",
      "Iteration 21, loss = 0.01198934\n",
      "Iteration 22, loss = 0.01132473\n",
      "Iteration 23, loss = 0.01077560\n",
      "Iteration 24, loss = 0.01028675\n",
      "Iteration 25, loss = 0.00978280\n",
      "Iteration 26, loss = 0.00938569\n",
      "Iteration 27, loss = 0.00892519\n",
      "Iteration 28, loss = 0.00858456\n",
      "Iteration 29, loss = 0.00819174\n",
      "Iteration 30, loss = 0.00782174\n",
      "Iteration 31, loss = 0.00752171\n",
      "Iteration 32, loss = 0.00704450\n",
      "Iteration 33, loss = 0.00677718\n",
      "Iteration 34, loss = 0.00646452\n",
      "Iteration 35, loss = 0.00623353\n",
      "Iteration 36, loss = 0.00614119\n",
      "Iteration 37, loss = 0.00587716\n",
      "Iteration 38, loss = 0.00575805\n",
      "Iteration 39, loss = 0.00545611\n",
      "Iteration 40, loss = 0.00529209\n",
      "Iteration 41, loss = 0.00512171\n",
      "Iteration 42, loss = 0.00495761\n",
      "Iteration 43, loss = 0.00491369\n",
      "Iteration 44, loss = 0.00480777\n",
      "Iteration 45, loss = 0.00469608\n",
      "Iteration 46, loss = 0.00455320\n",
      "Iteration 47, loss = 0.00445429\n",
      "Iteration 48, loss = 0.00436495\n",
      "Iteration 49, loss = 0.00430389\n",
      "Iteration 50, loss = 0.00426109\n",
      "Iteration 51, loss = 0.00414094\n",
      "Iteration 52, loss = 0.00396713\n",
      "Iteration 53, loss = 0.00395532\n",
      "Iteration 54, loss = 0.00382411\n",
      "Iteration 55, loss = 0.00377211\n",
      "Iteration 56, loss = 0.00375487\n",
      "Iteration 57, loss = 0.00357797\n",
      "Iteration 58, loss = 0.00332516\n",
      "Iteration 59, loss = 0.00320978\n",
      "Iteration 60, loss = 0.00310056\n",
      "Iteration 61, loss = 0.00296416\n",
      "Iteration 62, loss = 0.00280706\n",
      "Iteration 63, loss = 0.00267555\n",
      "Iteration 64, loss = 0.00265932\n",
      "Iteration 65, loss = 0.00255760\n",
      "Iteration 66, loss = 0.00253546\n",
      "Iteration 67, loss = 0.00250708\n",
      "Iteration 68, loss = 0.00230748\n",
      "Iteration 69, loss = 0.00229701\n",
      "Iteration 70, loss = 0.00225895\n",
      "Iteration 71, loss = 0.00222637\n",
      "Iteration 72, loss = 0.00215374\n",
      "Iteration 73, loss = 0.00220116\n",
      "Iteration 74, loss = 0.00223383\n",
      "Iteration 75, loss = 0.00214627\n",
      "Iteration 76, loss = 0.00206795\n",
      "Iteration 77, loss = 0.00209459\n",
      "Iteration 78, loss = 0.00193866\n",
      "Iteration 79, loss = 0.00195018\n",
      "Iteration 80, loss = 0.00191229\n",
      "Iteration 81, loss = 0.00193243\n",
      "Iteration 82, loss = 0.00195310\n",
      "Iteration 83, loss = 0.00183700\n",
      "Iteration 84, loss = 0.00182555\n",
      "Iteration 85, loss = 0.00188491\n",
      "Iteration 86, loss = 0.00179579\n",
      "Iteration 87, loss = 0.00169428\n",
      "Iteration 88, loss = 0.00170689\n",
      "Iteration 89, loss = 0.00167340\n",
      "Iteration 90, loss = 0.00172371\n",
      "Iteration 91, loss = 0.00167977\n",
      "Iteration 92, loss = 0.00160911\n",
      "Iteration 93, loss = 0.00164182\n",
      "Iteration 94, loss = 0.00160577\n",
      "Iteration 95, loss = 0.00154753\n",
      "Iteration 96, loss = 0.00156282\n",
      "Iteration 97, loss = 0.00154393\n",
      "Iteration 98, loss = 0.00149384\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████████████                                                                                                                                         | 1/9 [00:01<00:10,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.25887920\n",
      "Iteration 2, loss = 0.09297392\n",
      "Iteration 3, loss = 0.06245215\n",
      "Iteration 4, loss = 0.04994402\n",
      "Iteration 5, loss = 0.04250532\n",
      "Iteration 6, loss = 0.03715250\n",
      "Iteration 7, loss = 0.03330203\n",
      "Iteration 8, loss = 0.02994761\n",
      "Iteration 9, loss = 0.02732433\n",
      "Iteration 10, loss = 0.02519857\n",
      "Iteration 11, loss = 0.02331127\n",
      "Iteration 12, loss = 0.02178847\n",
      "Iteration 13, loss = 0.02051872\n",
      "Iteration 14, loss = 0.01915504\n",
      "Iteration 15, loss = 0.01816682\n",
      "Iteration 16, loss = 0.01707619\n",
      "Iteration 17, loss = 0.01626848\n",
      "Iteration 18, loss = 0.01571867\n",
      "Iteration 19, loss = 0.01503792\n",
      "Iteration 20, loss = 0.01456770\n",
      "Iteration 21, loss = 0.01404837\n",
      "Iteration 22, loss = 0.01361332\n",
      "Iteration 23, loss = 0.01316694\n",
      "Iteration 24, loss = 0.01277158\n",
      "Iteration 25, loss = 0.01246159\n",
      "Iteration 26, loss = 0.01186531\n",
      "Iteration 27, loss = 0.01153699\n",
      "Iteration 28, loss = 0.01092715\n",
      "Iteration 29, loss = 0.01055679\n",
      "Iteration 30, loss = 0.01009668\n",
      "Iteration 31, loss = 0.00965662\n",
      "Iteration 32, loss = 0.00931263\n",
      "Iteration 33, loss = 0.00892953\n",
      "Iteration 34, loss = 0.00869073\n",
      "Iteration 35, loss = 0.00811536\n",
      "Iteration 36, loss = 0.00770470\n",
      "Iteration 37, loss = 0.00733540\n",
      "Iteration 38, loss = 0.00694828\n",
      "Iteration 39, loss = 0.00651041\n",
      "Iteration 40, loss = 0.00617854\n",
      "Iteration 41, loss = 0.00586895\n",
      "Iteration 42, loss = 0.00562151\n",
      "Iteration 43, loss = 0.00536581\n",
      "Iteration 44, loss = 0.00511978\n",
      "Iteration 45, loss = 0.00489797\n",
      "Iteration 46, loss = 0.00473190\n",
      "Iteration 47, loss = 0.00452124\n",
      "Iteration 48, loss = 0.00437896\n",
      "Iteration 49, loss = 0.00420966\n",
      "Iteration 50, loss = 0.00396358\n",
      "Iteration 51, loss = 0.00379959\n",
      "Iteration 52, loss = 0.00365322\n",
      "Iteration 53, loss = 0.00357601\n",
      "Iteration 54, loss = 0.00332089\n",
      "Iteration 55, loss = 0.00331190\n",
      "Iteration 56, loss = 0.00331659\n",
      "Iteration 57, loss = 0.00321727\n",
      "Iteration 58, loss = 0.00315423\n",
      "Iteration 59, loss = 0.00302231\n",
      "Iteration 60, loss = 0.00296983\n",
      "Iteration 61, loss = 0.00285256\n",
      "Iteration 62, loss = 0.00280395\n",
      "Iteration 63, loss = 0.00269515\n",
      "Iteration 64, loss = 0.00263790\n",
      "Iteration 65, loss = 0.00256398\n",
      "Iteration 66, loss = 0.00262497\n",
      "Iteration 67, loss = 0.00249624\n",
      "Iteration 68, loss = 0.00248473\n",
      "Iteration 69, loss = 0.00237334\n",
      "Iteration 70, loss = 0.00234847\n",
      "Iteration 71, loss = 0.00230480\n",
      "Iteration 72, loss = 0.00230557\n",
      "Iteration 73, loss = 0.00221812\n",
      "Iteration 74, loss = 0.00219765\n",
      "Iteration 75, loss = 0.00213442\n",
      "Iteration 76, loss = 0.00212911\n",
      "Iteration 77, loss = 0.00212527\n",
      "Iteration 78, loss = 0.00214831\n",
      "Iteration 79, loss = 0.00215321\n",
      "Iteration 80, loss = 0.00205653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██████████████████████████████████▏                                                                                                                       | 2/9 [00:02<00:08,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.22579698\n",
      "Iteration 2, loss = 0.08897440\n",
      "Iteration 3, loss = 0.06089010\n",
      "Iteration 4, loss = 0.05012751\n",
      "Iteration 5, loss = 0.04382745\n",
      "Iteration 6, loss = 0.03881472\n",
      "Iteration 7, loss = 0.03428461\n",
      "Iteration 8, loss = 0.03039171\n",
      "Iteration 9, loss = 0.02706663\n",
      "Iteration 10, loss = 0.02434791\n",
      "Iteration 11, loss = 0.02206375\n",
      "Iteration 12, loss = 0.02005480\n",
      "Iteration 13, loss = 0.01855048\n",
      "Iteration 14, loss = 0.01708955\n",
      "Iteration 15, loss = 0.01598189\n",
      "Iteration 16, loss = 0.01488473\n",
      "Iteration 17, loss = 0.01399260\n",
      "Iteration 18, loss = 0.01316742\n",
      "Iteration 19, loss = 0.01244964\n",
      "Iteration 20, loss = 0.01169100\n",
      "Iteration 21, loss = 0.01100043\n",
      "Iteration 22, loss = 0.01037197\n",
      "Iteration 23, loss = 0.00991745\n",
      "Iteration 24, loss = 0.00952646\n",
      "Iteration 25, loss = 0.00905591\n",
      "Iteration 26, loss = 0.00873115\n",
      "Iteration 27, loss = 0.00829095\n",
      "Iteration 28, loss = 0.00792048\n",
      "Iteration 29, loss = 0.00770992\n",
      "Iteration 30, loss = 0.00734004\n",
      "Iteration 31, loss = 0.00691561\n",
      "Iteration 32, loss = 0.00666544\n",
      "Iteration 33, loss = 0.00644370\n",
      "Iteration 34, loss = 0.00626267\n",
      "Iteration 35, loss = 0.00598299\n",
      "Iteration 36, loss = 0.00589464\n",
      "Iteration 37, loss = 0.00575136\n",
      "Iteration 38, loss = 0.00556929\n",
      "Iteration 39, loss = 0.00542081\n",
      "Iteration 40, loss = 0.00524530\n",
      "Iteration 41, loss = 0.00518943\n",
      "Iteration 42, loss = 0.00504435\n",
      "Iteration 43, loss = 0.00494632\n",
      "Iteration 44, loss = 0.00476667\n",
      "Iteration 45, loss = 0.00461015\n",
      "Iteration 46, loss = 0.00453422\n",
      "Iteration 47, loss = 0.00447186\n",
      "Iteration 48, loss = 0.00420339\n",
      "Iteration 49, loss = 0.00413227\n",
      "Iteration 50, loss = 0.00405012\n",
      "Iteration 51, loss = 0.00385134\n",
      "Iteration 52, loss = 0.00372964\n",
      "Iteration 53, loss = 0.00360054\n",
      "Iteration 54, loss = 0.00344906\n",
      "Iteration 55, loss = 0.00331748\n",
      "Iteration 56, loss = 0.00318248\n",
      "Iteration 57, loss = 0.00303333\n",
      "Iteration 58, loss = 0.00293586\n",
      "Iteration 59, loss = 0.00286420\n",
      "Iteration 60, loss = 0.00279143\n",
      "Iteration 61, loss = 0.00259571\n",
      "Iteration 62, loss = 0.00257630\n",
      "Iteration 63, loss = 0.00256694\n",
      "Iteration 64, loss = 0.00253563\n",
      "Iteration 65, loss = 0.00241647\n",
      "Iteration 66, loss = 0.00233509\n",
      "Iteration 67, loss = 0.00229790\n",
      "Iteration 68, loss = 0.00230036\n",
      "Iteration 69, loss = 0.00221847\n",
      "Iteration 70, loss = 0.00219130\n",
      "Iteration 71, loss = 0.00224890\n",
      "Iteration 72, loss = 0.00213459\n",
      "Iteration 73, loss = 0.00205995\n",
      "Iteration 74, loss = 0.00200908\n",
      "Iteration 75, loss = 0.00199289\n",
      "Iteration 76, loss = 0.00199046\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████████████████████████████████████▎                                                                                                      | 3/9 [00:03<00:06,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.25497578\n",
      "Iteration 2, loss = 0.11716241\n",
      "Iteration 3, loss = 0.06916969\n",
      "Iteration 4, loss = 0.05380464\n",
      "Iteration 5, loss = 0.04647053\n",
      "Iteration 6, loss = 0.04208158\n",
      "Iteration 7, loss = 0.03790467\n",
      "Iteration 8, loss = 0.03439091\n",
      "Iteration 9, loss = 0.03122523\n",
      "Iteration 10, loss = 0.02842590\n",
      "Iteration 11, loss = 0.02630258\n",
      "Iteration 12, loss = 0.02422275\n",
      "Iteration 13, loss = 0.02252130\n",
      "Iteration 14, loss = 0.02087580\n",
      "Iteration 15, loss = 0.01948581\n",
      "Iteration 16, loss = 0.01844895\n",
      "Iteration 17, loss = 0.01735638\n",
      "Iteration 18, loss = 0.01639853\n",
      "Iteration 19, loss = 0.01541848\n",
      "Iteration 20, loss = 0.01462111\n",
      "Iteration 21, loss = 0.01391085\n",
      "Iteration 22, loss = 0.01330484\n",
      "Iteration 23, loss = 0.01264858\n",
      "Iteration 24, loss = 0.01198887\n",
      "Iteration 25, loss = 0.01137247\n",
      "Iteration 26, loss = 0.01078692\n",
      "Iteration 27, loss = 0.01020606\n",
      "Iteration 28, loss = 0.00974266\n",
      "Iteration 29, loss = 0.00916337\n",
      "Iteration 30, loss = 0.00864837\n",
      "Iteration 31, loss = 0.00814452\n",
      "Iteration 32, loss = 0.00766854\n",
      "Iteration 33, loss = 0.00743148\n",
      "Iteration 34, loss = 0.00724128\n",
      "Iteration 35, loss = 0.00669821\n",
      "Iteration 36, loss = 0.00633335\n",
      "Iteration 37, loss = 0.00607145\n",
      "Iteration 38, loss = 0.00590814\n",
      "Iteration 39, loss = 0.00570823\n",
      "Iteration 40, loss = 0.00555588\n",
      "Iteration 41, loss = 0.00525264\n",
      "Iteration 42, loss = 0.00518507\n",
      "Iteration 43, loss = 0.00493161\n",
      "Iteration 44, loss = 0.00466256\n",
      "Iteration 45, loss = 0.00443355\n",
      "Iteration 46, loss = 0.00429609\n",
      "Iteration 47, loss = 0.00423333\n",
      "Iteration 48, loss = 0.00410527\n",
      "Iteration 49, loss = 0.00399579\n",
      "Iteration 50, loss = 0.00385910\n",
      "Iteration 51, loss = 0.00375604\n",
      "Iteration 52, loss = 0.00356861\n",
      "Iteration 53, loss = 0.00351887\n",
      "Iteration 54, loss = 0.00335944\n",
      "Iteration 55, loss = 0.00323188\n",
      "Iteration 56, loss = 0.00310936\n",
      "Iteration 57, loss = 0.00299647\n",
      "Iteration 58, loss = 0.00292251\n",
      "Iteration 59, loss = 0.00291209\n",
      "Iteration 60, loss = 0.00287797\n",
      "Iteration 61, loss = 0.00278112\n",
      "Iteration 62, loss = 0.00277508\n",
      "Iteration 63, loss = 0.00266052\n",
      "Iteration 64, loss = 0.00266762\n",
      "Iteration 65, loss = 0.00254103\n",
      "Iteration 66, loss = 0.00250326\n",
      "Iteration 67, loss = 0.00253147\n",
      "Iteration 68, loss = 0.00248952\n",
      "Iteration 69, loss = 0.00240588\n",
      "Iteration 70, loss = 0.00233908\n",
      "Iteration 71, loss = 0.00239397\n",
      "Iteration 72, loss = 0.00232767\n",
      "Iteration 73, loss = 0.00228553\n",
      "Iteration 74, loss = 0.00226183\n",
      "Iteration 75, loss = 0.00225330\n",
      "Iteration 76, loss = 0.00205742\n",
      "Iteration 77, loss = 0.00208253\n",
      "Iteration 78, loss = 0.00203754\n",
      "Iteration 79, loss = 0.00196592\n",
      "Iteration 80, loss = 0.00194675\n",
      "Iteration 81, loss = 0.00188782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████████████████████████████████████▍                                                                                     | 4/9 [00:04<00:05,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 82, loss = 0.00189168\n",
      "Iteration 83, loss = 0.00185611\n",
      "Iteration 84, loss = 0.00180061\n",
      "Iteration 85, loss = 0.00178149\n",
      "Iteration 86, loss = 0.00177009\n",
      "Iteration 87, loss = 0.00173151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.30868643\n",
      "Iteration 2, loss = 0.14157841\n",
      "Iteration 3, loss = 0.08972725\n",
      "Iteration 4, loss = 0.06952119\n",
      "Iteration 5, loss = 0.05485031\n",
      "Iteration 6, loss = 0.04653647\n",
      "Iteration 7, loss = 0.04140200\n",
      "Iteration 8, loss = 0.03733086\n",
      "Iteration 9, loss = 0.03477641\n",
      "Iteration 10, loss = 0.03257224\n",
      "Iteration 11, loss = 0.03088236\n",
      "Iteration 12, loss = 0.02939836\n",
      "Iteration 13, loss = 0.02772315\n",
      "Iteration 14, loss = 0.02628284\n",
      "Iteration 15, loss = 0.02477926\n",
      "Iteration 16, loss = 0.02341581\n",
      "Iteration 17, loss = 0.02216274\n",
      "Iteration 18, loss = 0.02085037\n",
      "Iteration 19, loss = 0.02086086\n",
      "Iteration 20, loss = 0.01909894\n",
      "Iteration 21, loss = 0.01821392\n",
      "Iteration 22, loss = 0.01751388\n",
      "Iteration 23, loss = 0.01665718\n",
      "Iteration 24, loss = 0.01624553\n",
      "Iteration 25, loss = 0.01538225\n",
      "Iteration 26, loss = 0.01466281\n",
      "Iteration 27, loss = 0.01401397\n",
      "Iteration 28, loss = 0.01320823\n",
      "Iteration 29, loss = 0.01266499\n",
      "Iteration 30, loss = 0.01224160\n",
      "Iteration 31, loss = 0.01139259\n",
      "Iteration 32, loss = 0.01121829\n",
      "Iteration 33, loss = 0.01045852\n",
      "Iteration 34, loss = 0.00975590\n",
      "Iteration 35, loss = 0.00935541\n",
      "Iteration 36, loss = 0.00902750\n",
      "Iteration 37, loss = 0.00849352\n",
      "Iteration 38, loss = 0.00783961\n",
      "Iteration 39, loss = 0.00745884\n",
      "Iteration 40, loss = 0.00717499\n",
      "Iteration 41, loss = 0.00709802\n",
      "Iteration 42, loss = 0.00659021\n",
      "Iteration 43, loss = 0.00646156\n",
      "Iteration 44, loss = 0.00614068\n",
      "Iteration 45, loss = 0.00582106\n",
      "Iteration 46, loss = 0.00582231\n",
      "Iteration 47, loss = 0.00563624\n",
      "Iteration 48, loss = 0.00545786\n",
      "Iteration 49, loss = 0.00517996\n",
      "Iteration 50, loss = 0.00503343\n",
      "Iteration 51, loss = 0.00495505\n",
      "Iteration 52, loss = 0.00486137\n",
      "Iteration 53, loss = 0.00487768\n",
      "Iteration 54, loss = 0.00502339\n",
      "Iteration 55, loss = 0.00452390\n",
      "Iteration 56, loss = 0.00435911\n",
      "Iteration 57, loss = 0.00433458\n",
      "Iteration 58, loss = 0.00449593\n",
      "Iteration 59, loss = 0.00442534\n",
      "Iteration 60, loss = 0.00419080\n",
      "Iteration 61, loss = 0.00464625\n",
      "Iteration 62, loss = 0.00426502\n",
      "Iteration 63, loss = 0.00395535\n",
      "Iteration 64, loss = 0.00393387\n",
      "Iteration 65, loss = 0.00417930\n",
      "Iteration 66, loss = 0.00397662\n",
      "Iteration 67, loss = 0.00363960\n",
      "Iteration 68, loss = 0.00341103\n",
      "Iteration 69, loss = 0.00329954\n",
      "Iteration 70, loss = 0.00332969\n",
      "Iteration 71, loss = 0.00325165\n",
      "Iteration 72, loss = 0.00323503\n",
      "Iteration 73, loss = 0.00313189\n",
      "Iteration 74, loss = 0.00304465\n",
      "Iteration 75, loss = 0.00293775\n",
      "Iteration 76, loss = 0.00298255\n",
      "Iteration 77, loss = 0.00313544\n",
      "Iteration 78, loss = 0.00335258\n",
      "Iteration 79, loss = 0.00314303\n",
      "Iteration 80, loss = 0.00287807\n",
      "Iteration 81, loss = 0.00273488\n",
      "Iteration 82, loss = 0.00298709\n",
      "Iteration 83, loss = 0.00318493\n",
      "Iteration 84, loss = 0.00317128\n",
      "Iteration 85, loss = 0.00280711\n",
      "Iteration 86, loss = 0.00257247\n",
      "Iteration 87, loss = 0.00261296\n",
      "Iteration 88, loss = 0.00246171\n",
      "Iteration 89, loss = 0.00264807\n",
      "Iteration 90, loss = 0.00247356\n",
      "Iteration 91, loss = 0.00233823\n",
      "Iteration 92, loss = 0.00251723\n",
      "Iteration 93, loss = 0.00244992\n",
      "Iteration 94, loss = 0.00231905\n",
      "Iteration 95, loss = 0.00219739\n",
      "Iteration 96, loss = 0.00221535\n",
      "Iteration 97, loss = 0.00253013\n",
      "Iteration 98, loss = 0.00291734\n",
      "Iteration 99, loss = 0.00242861\n",
      "Iteration 100, loss = 0.00225083\n",
      "Iteration 101, loss = 0.00234260\n",
      "Iteration 102, loss = 0.00220769\n",
      "Iteration 103, loss = 0.00210504\n",
      "Iteration 104, loss = 0.00212572\n",
      "Iteration 105, loss = 0.00204816\n",
      "Iteration 106, loss = 0.00215099\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 5/9 [00:05<00:04,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.25699597\n",
      "Iteration 2, loss = 0.11958038\n",
      "Iteration 3, loss = 0.08112575\n",
      "Iteration 4, loss = 0.06731875\n",
      "Iteration 5, loss = 0.05902079\n",
      "Iteration 6, loss = 0.05292262\n",
      "Iteration 7, loss = 0.04769570\n",
      "Iteration 8, loss = 0.04306919\n",
      "Iteration 9, loss = 0.03906615\n",
      "Iteration 10, loss = 0.03546248\n",
      "Iteration 11, loss = 0.03251506\n",
      "Iteration 12, loss = 0.02989648\n",
      "Iteration 13, loss = 0.02781081\n",
      "Iteration 14, loss = 0.02588523\n",
      "Iteration 15, loss = 0.02387235\n",
      "Iteration 16, loss = 0.02215875\n",
      "Iteration 17, loss = 0.02064651\n",
      "Iteration 18, loss = 0.01941246\n",
      "Iteration 19, loss = 0.01808071\n",
      "Iteration 20, loss = 0.01691340\n",
      "Iteration 21, loss = 0.01602018\n",
      "Iteration 22, loss = 0.01510186\n",
      "Iteration 23, loss = 0.01424787\n",
      "Iteration 24, loss = 0.01353111\n",
      "Iteration 25, loss = 0.01283374\n",
      "Iteration 26, loss = 0.01210678\n",
      "Iteration 27, loss = 0.01150705\n",
      "Iteration 28, loss = 0.01096361\n",
      "Iteration 29, loss = 0.01050039\n",
      "Iteration 30, loss = 0.01004525\n",
      "Iteration 31, loss = 0.00957523\n",
      "Iteration 32, loss = 0.00926722\n",
      "Iteration 33, loss = 0.00890514\n",
      "Iteration 34, loss = 0.00872823\n",
      "Iteration 35, loss = 0.00828328\n",
      "Iteration 36, loss = 0.00790787\n",
      "Iteration 37, loss = 0.00765365\n",
      "Iteration 38, loss = 0.00740837\n",
      "Iteration 39, loss = 0.00711403\n",
      "Iteration 40, loss = 0.00674392\n",
      "Iteration 41, loss = 0.00654722\n",
      "Iteration 42, loss = 0.00631801\n",
      "Iteration 43, loss = 0.00589792\n",
      "Iteration 44, loss = 0.00560725\n",
      "Iteration 45, loss = 0.00535000\n",
      "Iteration 46, loss = 0.00505498\n",
      "Iteration 47, loss = 0.00471404\n",
      "Iteration 48, loss = 0.00448321\n",
      "Iteration 49, loss = 0.00437300\n",
      "Iteration 50, loss = 0.00426894\n",
      "Iteration 51, loss = 0.00408002\n",
      "Iteration 52, loss = 0.00391995\n",
      "Iteration 53, loss = 0.00379152\n",
      "Iteration 54, loss = 0.00376407\n",
      "Iteration 55, loss = 0.00368633\n",
      "Iteration 56, loss = 0.00363793\n",
      "Iteration 57, loss = 0.00364534\n",
      "Iteration 58, loss = 0.00346103\n",
      "Iteration 59, loss = 0.00345623\n",
      "Iteration 60, loss = 0.00340573\n",
      "Iteration 61, loss = 0.00337487\n",
      "Iteration 62, loss = 0.00318783\n",
      "Iteration 63, loss = 0.00309145\n",
      "Iteration 64, loss = 0.00301797\n",
      "Iteration 65, loss = 0.00301320\n",
      "Iteration 66, loss = 0.00314048\n",
      "Iteration 67, loss = 0.00296381\n",
      "Iteration 68, loss = 0.00292289\n",
      "Iteration 69, loss = 0.00287739\n",
      "Iteration 70, loss = 0.00276294\n",
      "Iteration 71, loss = 0.00278594\n",
      "Iteration 72, loss = 0.00272691\n",
      "Iteration 73, loss = 0.00271940\n",
      "Iteration 74, loss = 0.00271171\n",
      "Iteration 75, loss = 0.00262759\n",
      "Iteration 76, loss = 0.00252911\n",
      "Iteration 77, loss = 0.00252473\n",
      "Iteration 78, loss = 0.00246888\n",
      "Iteration 79, loss = 0.00255146\n",
      "Iteration 80, loss = 0.00234029\n",
      "Iteration 81, loss = 0.00233424\n",
      "Iteration 82, loss = 0.00230830\n",
      "Iteration 83, loss = 0.00224810\n",
      "Iteration 84, loss = 0.00238519\n",
      "Iteration 85, loss = 0.00239515\n",
      "Iteration 86, loss = 0.00222895\n",
      "Iteration 87, loss = 0.00215496\n",
      "Iteration 88, loss = 0.00209389\n",
      "Iteration 89, loss = 0.00203715\n",
      "Iteration 90, loss = 0.00204878\n",
      "Iteration 91, loss = 0.00205122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 6/9 [00:06<00:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.33841870\n",
      "Iteration 2, loss = 0.13990839\n",
      "Iteration 3, loss = 0.08197084\n",
      "Iteration 4, loss = 0.06207584\n",
      "Iteration 5, loss = 0.05187210\n",
      "Iteration 6, loss = 0.04575682\n",
      "Iteration 7, loss = 0.04101321\n",
      "Iteration 8, loss = 0.03726834\n",
      "Iteration 9, loss = 0.03446222\n",
      "Iteration 10, loss = 0.03203239\n",
      "Iteration 11, loss = 0.03011405\n",
      "Iteration 12, loss = 0.02853355\n",
      "Iteration 13, loss = 0.02707355\n",
      "Iteration 14, loss = 0.02590696\n",
      "Iteration 15, loss = 0.02459476\n",
      "Iteration 16, loss = 0.02356450\n",
      "Iteration 17, loss = 0.02258332\n",
      "Iteration 18, loss = 0.02174340\n",
      "Iteration 19, loss = 0.02093133\n",
      "Iteration 20, loss = 0.02014192\n",
      "Iteration 21, loss = 0.01949502\n",
      "Iteration 22, loss = 0.01873093\n",
      "Iteration 23, loss = 0.01791055\n",
      "Iteration 24, loss = 0.01714095\n",
      "Iteration 25, loss = 0.01640601\n",
      "Iteration 26, loss = 0.01555402\n",
      "Iteration 27, loss = 0.01481363\n",
      "Iteration 28, loss = 0.01422144\n",
      "Iteration 29, loss = 0.01349894\n",
      "Iteration 30, loss = 0.01303616\n",
      "Iteration 31, loss = 0.01272480\n",
      "Iteration 32, loss = 0.01215616\n",
      "Iteration 33, loss = 0.01170709\n",
      "Iteration 34, loss = 0.01131196\n",
      "Iteration 35, loss = 0.01092696\n",
      "Iteration 36, loss = 0.01064403\n",
      "Iteration 37, loss = 0.01037668\n",
      "Iteration 38, loss = 0.01006443\n",
      "Iteration 39, loss = 0.00969468\n",
      "Iteration 40, loss = 0.00945147\n",
      "Iteration 41, loss = 0.00924450\n",
      "Iteration 42, loss = 0.00907485\n",
      "Iteration 43, loss = 0.00877800\n",
      "Iteration 44, loss = 0.00867420\n",
      "Iteration 45, loss = 0.00829033\n",
      "Iteration 46, loss = 0.00807758\n",
      "Iteration 47, loss = 0.00792869\n",
      "Iteration 48, loss = 0.00769386\n",
      "Iteration 49, loss = 0.00758431\n",
      "Iteration 50, loss = 0.00734814\n",
      "Iteration 51, loss = 0.00731935\n",
      "Iteration 52, loss = 0.00706137\n",
      "Iteration 53, loss = 0.00692832\n",
      "Iteration 54, loss = 0.00672753\n",
      "Iteration 55, loss = 0.00660415\n",
      "Iteration 56, loss = 0.00644897\n",
      "Iteration 57, loss = 0.00629542\n",
      "Iteration 58, loss = 0.00607096\n",
      "Iteration 59, loss = 0.00599466\n",
      "Iteration 60, loss = 0.00584645\n",
      "Iteration 61, loss = 0.00560426\n",
      "Iteration 62, loss = 0.00545828\n",
      "Iteration 63, loss = 0.00523227\n",
      "Iteration 64, loss = 0.00511227\n",
      "Iteration 65, loss = 0.00499768\n",
      "Iteration 66, loss = 0.00479255\n",
      "Iteration 67, loss = 0.00461276\n",
      "Iteration 68, loss = 0.00438519\n",
      "Iteration 69, loss = 0.00423408\n",
      "Iteration 70, loss = 0.00406538\n",
      "Iteration 71, loss = 0.00398043\n",
      "Iteration 72, loss = 0.00383554\n",
      "Iteration 73, loss = 0.00372559\n",
      "Iteration 74, loss = 0.00360883\n",
      "Iteration 75, loss = 0.00347427\n",
      "Iteration 76, loss = 0.00339751\n",
      "Iteration 77, loss = 0.00328741\n",
      "Iteration 78, loss = 0.00319569\n",
      "Iteration 79, loss = 0.00309595\n",
      "Iteration 80, loss = 0.00299296\n",
      "Iteration 81, loss = 0.00303895\n",
      "Iteration 82, loss = 0.00291719\n",
      "Iteration 83, loss = 0.00290455\n",
      "Iteration 84, loss = 0.00284317\n",
      "Iteration 85, loss = 0.00277099\n",
      "Iteration 86, loss = 0.00268617\n",
      "Iteration 87, loss = 0.00253380\n",
      "Iteration 88, loss = 0.00252904\n",
      "Iteration 89, loss = 0.00250382\n",
      "Iteration 90, loss = 0.00254351\n",
      "Iteration 91, loss = 0.00258413\n",
      "Iteration 92, loss = 0.00247033\n",
      "Iteration 93, loss = 0.00237862\n",
      "Iteration 94, loss = 0.00233189\n",
      "Iteration 95, loss = 0.00233802\n",
      "Iteration 96, loss = 0.00237853\n",
      "Iteration 97, loss = 0.00228727\n",
      "Iteration 98, loss = 0.00233668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 7/9 [00:07<00:02,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39166020\n",
      "Iteration 2, loss = 0.17630904\n",
      "Iteration 3, loss = 0.10277258\n",
      "Iteration 4, loss = 0.07394341\n",
      "Iteration 5, loss = 0.06225724\n",
      "Iteration 6, loss = 0.05514162\n",
      "Iteration 7, loss = 0.04945804\n",
      "Iteration 8, loss = 0.04420466\n",
      "Iteration 9, loss = 0.03988150\n",
      "Iteration 10, loss = 0.03630240\n",
      "Iteration 11, loss = 0.03338643\n",
      "Iteration 12, loss = 0.03114349\n",
      "Iteration 13, loss = 0.02929185\n",
      "Iteration 14, loss = 0.02754592\n",
      "Iteration 15, loss = 0.02608447\n",
      "Iteration 16, loss = 0.02471311\n",
      "Iteration 17, loss = 0.02350590\n",
      "Iteration 18, loss = 0.02242012\n",
      "Iteration 19, loss = 0.02131596\n",
      "Iteration 20, loss = 0.02049927\n",
      "Iteration 21, loss = 0.01966080\n",
      "Iteration 22, loss = 0.01905705\n",
      "Iteration 23, loss = 0.01833450\n",
      "Iteration 24, loss = 0.01759951\n",
      "Iteration 25, loss = 0.01691429\n",
      "Iteration 26, loss = 0.01635039\n",
      "Iteration 27, loss = 0.01572621\n",
      "Iteration 28, loss = 0.01517434\n",
      "Iteration 29, loss = 0.01469656\n",
      "Iteration 30, loss = 0.01423733\n",
      "Iteration 31, loss = 0.01369379\n",
      "Iteration 32, loss = 0.01315618\n",
      "Iteration 33, loss = 0.01264174\n",
      "Iteration 34, loss = 0.01228791\n",
      "Iteration 35, loss = 0.01180561\n",
      "Iteration 36, loss = 0.01135696\n",
      "Iteration 37, loss = 0.01092373\n",
      "Iteration 38, loss = 0.01056414\n",
      "Iteration 39, loss = 0.01014798\n",
      "Iteration 40, loss = 0.00973013\n",
      "Iteration 41, loss = 0.00947667\n",
      "Iteration 42, loss = 0.00918641\n",
      "Iteration 43, loss = 0.00888111\n",
      "Iteration 44, loss = 0.00879033\n",
      "Iteration 45, loss = 0.00842061\n",
      "Iteration 46, loss = 0.00814145\n",
      "Iteration 47, loss = 0.00790109\n",
      "Iteration 48, loss = 0.00762276\n",
      "Iteration 49, loss = 0.00742937\n",
      "Iteration 50, loss = 0.00730538\n",
      "Iteration 51, loss = 0.00697577\n",
      "Iteration 52, loss = 0.00687295\n",
      "Iteration 53, loss = 0.00654674\n",
      "Iteration 54, loss = 0.00632201\n",
      "Iteration 55, loss = 0.00614963\n",
      "Iteration 56, loss = 0.00592031\n",
      "Iteration 57, loss = 0.00568871\n",
      "Iteration 58, loss = 0.00555804\n",
      "Iteration 59, loss = 0.00528413\n",
      "Iteration 60, loss = 0.00509393\n",
      "Iteration 61, loss = 0.00507409\n",
      "Iteration 62, loss = 0.00477543\n",
      "Iteration 63, loss = 0.00466524\n",
      "Iteration 64, loss = 0.00450814\n",
      "Iteration 65, loss = 0.00429910\n",
      "Iteration 66, loss = 0.00422523\n",
      "Iteration 67, loss = 0.00409149\n",
      "Iteration 68, loss = 0.00403956\n",
      "Iteration 69, loss = 0.00398401\n",
      "Iteration 70, loss = 0.00377936\n",
      "Iteration 71, loss = 0.00372153\n",
      "Iteration 72, loss = 0.00372492\n",
      "Iteration 73, loss = 0.00358369\n",
      "Iteration 74, loss = 0.00351318\n",
      "Iteration 75, loss = 0.00348412\n",
      "Iteration 76, loss = 0.00347462\n",
      "Iteration 77, loss = 0.00341315\n",
      "Iteration 78, loss = 0.00332494\n",
      "Iteration 79, loss = 0.00327194\n",
      "Iteration 80, loss = 0.00316001\n",
      "Iteration 81, loss = 0.00312138\n",
      "Iteration 82, loss = 0.00304623\n",
      "Iteration 83, loss = 0.00303967\n",
      "Iteration 84, loss = 0.00299123\n",
      "Iteration 85, loss = 0.00294231\n",
      "Iteration 86, loss = 0.00294534\n",
      "Iteration 87, loss = 0.00283085\n",
      "Iteration 88, loss = 0.00284193\n",
      "Iteration 89, loss = 0.00292937\n",
      "Iteration 90, loss = 0.00289252\n",
      "Iteration 91, loss = 0.00272378\n",
      "Iteration 92, loss = 0.00264632\n",
      "Iteration 93, loss = 0.00267005\n",
      "Iteration 94, loss = 0.00265071\n",
      "Iteration 95, loss = 0.00253932\n",
      "Iteration 96, loss = 0.00254117\n",
      "Iteration 97, loss = 0.00250938\n",
      "Iteration 98, loss = 0.00243579\n",
      "Iteration 99, loss = 0.00235985\n",
      "Iteration 100, loss = 0.00232128\n",
      "Iteration 101, loss = 0.00230306\n",
      "Iteration 102, loss = 0.00235474\n",
      "Iteration 103, loss = 0.00227649\n",
      "Iteration 104, loss = 0.00223539\n",
      "Iteration 105, loss = 0.00220894\n",
      "Iteration 106, loss = 0.00219654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 8/9 [00:08<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39961796\n",
      "Iteration 2, loss = 0.20156693\n",
      "Iteration 3, loss = 0.11526768\n",
      "Iteration 4, loss = 0.07786102\n",
      "Iteration 5, loss = 0.05983971\n",
      "Iteration 6, loss = 0.04970007\n",
      "Iteration 7, loss = 0.04357976\n",
      "Iteration 8, loss = 0.03908520\n",
      "Iteration 9, loss = 0.03600764\n",
      "Iteration 10, loss = 0.03350290\n",
      "Iteration 11, loss = 0.03151192\n",
      "Iteration 12, loss = 0.02984781\n",
      "Iteration 13, loss = 0.02810890\n",
      "Iteration 14, loss = 0.02660363\n",
      "Iteration 15, loss = 0.02521474\n",
      "Iteration 16, loss = 0.02388569\n",
      "Iteration 17, loss = 0.02261568\n",
      "Iteration 18, loss = 0.02125592\n",
      "Iteration 19, loss = 0.02018220\n",
      "Iteration 20, loss = 0.01930029\n",
      "Iteration 21, loss = 0.01862894\n",
      "Iteration 22, loss = 0.01781841\n",
      "Iteration 23, loss = 0.01709026\n",
      "Iteration 24, loss = 0.01645556\n",
      "Iteration 25, loss = 0.01611679\n",
      "Iteration 26, loss = 0.01546617\n",
      "Iteration 27, loss = 0.01496510\n",
      "Iteration 28, loss = 0.01450458\n",
      "Iteration 29, loss = 0.01418662\n",
      "Iteration 30, loss = 0.01363095\n",
      "Iteration 31, loss = 0.01326445\n",
      "Iteration 32, loss = 0.01292655\n",
      "Iteration 33, loss = 0.01258383\n",
      "Iteration 34, loss = 0.01240709\n",
      "Iteration 35, loss = 0.01214504\n",
      "Iteration 36, loss = 0.01172824\n",
      "Iteration 37, loss = 0.01152223\n",
      "Iteration 38, loss = 0.01163044\n",
      "Iteration 39, loss = 0.01104711\n",
      "Iteration 40, loss = 0.01087808\n",
      "Iteration 41, loss = 0.01050085\n",
      "Iteration 42, loss = 0.01031013\n",
      "Iteration 43, loss = 0.01027643\n",
      "Iteration 44, loss = 0.00994396\n",
      "Iteration 45, loss = 0.00973709\n",
      "Iteration 46, loss = 0.00938009\n",
      "Iteration 47, loss = 0.00916148\n",
      "Iteration 48, loss = 0.00905574\n",
      "Iteration 49, loss = 0.00882018\n",
      "Iteration 50, loss = 0.00850370\n",
      "Iteration 51, loss = 0.00838846\n",
      "Iteration 52, loss = 0.00815416\n",
      "Iteration 53, loss = 0.00797975\n",
      "Iteration 54, loss = 0.00767277\n",
      "Iteration 55, loss = 0.00747822\n",
      "Iteration 56, loss = 0.00729090\n",
      "Iteration 57, loss = 0.00706156\n",
      "Iteration 58, loss = 0.00700754\n",
      "Iteration 59, loss = 0.00688515\n",
      "Iteration 60, loss = 0.00671845\n",
      "Iteration 61, loss = 0.00655849\n",
      "Iteration 62, loss = 0.00642272\n",
      "Iteration 63, loss = 0.00636172\n",
      "Iteration 64, loss = 0.00628742\n",
      "Iteration 65, loss = 0.00596502\n",
      "Iteration 66, loss = 0.00569171\n",
      "Iteration 67, loss = 0.00556139\n",
      "Iteration 68, loss = 0.00539716\n",
      "Iteration 69, loss = 0.00526341\n",
      "Iteration 70, loss = 0.00521063\n",
      "Iteration 71, loss = 0.00505032\n",
      "Iteration 72, loss = 0.00491463\n",
      "Iteration 73, loss = 0.00473734\n",
      "Iteration 74, loss = 0.00476536\n",
      "Iteration 75, loss = 0.00460299\n",
      "Iteration 76, loss = 0.00438491\n",
      "Iteration 77, loss = 0.00435242\n",
      "Iteration 78, loss = 0.00436822\n",
      "Iteration 79, loss = 0.00420993\n",
      "Iteration 80, loss = 0.00408434\n",
      "Iteration 81, loss = 0.00396478\n",
      "Iteration 82, loss = 0.00392710\n",
      "Iteration 83, loss = 0.00384328\n",
      "Iteration 84, loss = 0.00390935\n",
      "Iteration 85, loss = 0.00380592\n",
      "Iteration 86, loss = 0.00386634\n",
      "Iteration 87, loss = 0.00374819\n",
      "Iteration 88, loss = 0.00371814\n",
      "Iteration 89, loss = 0.00354858\n",
      "Iteration 90, loss = 0.00350278\n",
      "Iteration 91, loss = 0.00347789\n",
      "Iteration 92, loss = 0.00337830\n",
      "Iteration 93, loss = 0.00333594\n",
      "Iteration 94, loss = 0.00338858\n",
      "Iteration 95, loss = 0.00333149\n",
      "Iteration 96, loss = 0.00320720\n",
      "Iteration 97, loss = 0.00321151\n",
      "Iteration 98, loss = 0.00303645\n",
      "Iteration 99, loss = 0.00299786\n",
      "Iteration 100, loss = 0.00309747\n",
      "Iteration 101, loss = 0.00314782\n",
      "Iteration 102, loss = 0.00301783\n",
      "Iteration 103, loss = 0.00300208\n",
      "Iteration 104, loss = 0.00288585\n",
      "Iteration 105, loss = 0.00282323\n",
      "Iteration 106, loss = 0.00274486\n",
      "Iteration 107, loss = 0.00271135\n",
      "Iteration 108, loss = 0.00262530\n",
      "Iteration 109, loss = 0.00267971\n",
      "Iteration 110, loss = 0.00261123\n",
      "Iteration 111, loss = 0.00251573\n",
      "Iteration 112, loss = 0.00248599\n",
      "Iteration 113, loss = 0.00247349\n",
      "Iteration 114, loss = 0.00262101\n",
      "Iteration 115, loss = 0.00243077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:09<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "zero_measures = index_series(\n",
    "    quartal_data.values,\n",
    "    window=windowq,\n",
    "    period=4,\n",
    "    model=lambda:ZeroModel(),\n",
    "    step=1,\n",
    "    noise_iters=0,\n",
    "    global_offset=5\n",
    ")\n",
    "\n",
    "knn_measures = index_series(\n",
    "    quartal_data.values,\n",
    "    window=windowq,\n",
    "    period=4,\n",
    "    model=lambda:AutoencKnnWrapper(reg, KNeighborsRegressor(n_neighbors=50)),\n",
    "    step=1,\n",
    "    noise_iters=0,\n",
    "    global_offset=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'p_value': 0.009721034112722968,\n",
       "  'mu': -0.29520040101110406,\n",
       "  'sigma': 0.6253406075683534,\n",
       "  'win_rate': 0.2706552706552707},\n",
       " 2.364627628035963,\n",
       " 2.1699269083244617)"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_z_test(knn_measures[1], zero_measures[1]), np.mean(knn_measures[1]), np.mean(zero_measures[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'p_value': 0.04211387004915458,\n",
       "  'mu': 0.13845365412778213,\n",
       "  'sigma': 0.37312755743785997,\n",
       "  'win_rate': 0.7692307692307693},\n",
       " 0.2304385966949221,\n",
       " 0.27897311971554184)"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_z_test(knn_measures[0], zero_measures[0]), np.mean(knn_measures[0]), np.mean(zero_measures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2304385966949221, 0.27897311971554184)"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_month = pd.read_csv(\"fred_md.csv\", skiprows=range(1,2), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPI</th>\n",
       "      <th>W875RX1</th>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <th>CMRMTSPLx</th>\n",
       "      <th>RETAILx</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>IPFPNSS</th>\n",
       "      <th>IPFINAL</th>\n",
       "      <th>IPCONGD</th>\n",
       "      <th>IPDCONGD</th>\n",
       "      <th>...</th>\n",
       "      <th>DNDGRG3M086SBEA</th>\n",
       "      <th>DSERRG3M086SBEA</th>\n",
       "      <th>CES0600000008</th>\n",
       "      <th>CES2000000008</th>\n",
       "      <th>CES3000000008</th>\n",
       "      <th>UMCSENTx</th>\n",
       "      <th>DTCOLNVHFNM</th>\n",
       "      <th>DTCTHFNM</th>\n",
       "      <th>INVEST</th>\n",
       "      <th>VIXCLSx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sasdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1/1/1959</th>\n",
       "      <td>2442.158</td>\n",
       "      <td>2293.2</td>\n",
       "      <td>17.272</td>\n",
       "      <td>292266.4261</td>\n",
       "      <td>18235.77392</td>\n",
       "      <td>22.0151</td>\n",
       "      <td>23.3984</td>\n",
       "      <td>22.2848</td>\n",
       "      <td>31.5847</td>\n",
       "      <td>18.6861</td>\n",
       "      <td>...</td>\n",
       "      <td>17.791</td>\n",
       "      <td>11.326</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6476.0</td>\n",
       "      <td>12298.0</td>\n",
       "      <td>84.2043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/1/1959</th>\n",
       "      <td>2451.778</td>\n",
       "      <td>2301.5</td>\n",
       "      <td>17.452</td>\n",
       "      <td>294424.7425</td>\n",
       "      <td>18369.56308</td>\n",
       "      <td>22.4463</td>\n",
       "      <td>23.7142</td>\n",
       "      <td>22.4778</td>\n",
       "      <td>31.8164</td>\n",
       "      <td>18.7842</td>\n",
       "      <td>...</td>\n",
       "      <td>17.798</td>\n",
       "      <td>11.343</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6476.0</td>\n",
       "      <td>12298.0</td>\n",
       "      <td>83.5280</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3/1/1959</th>\n",
       "      <td>2467.594</td>\n",
       "      <td>2318.5</td>\n",
       "      <td>17.617</td>\n",
       "      <td>293418.6704</td>\n",
       "      <td>18523.05762</td>\n",
       "      <td>22.7696</td>\n",
       "      <td>23.8577</td>\n",
       "      <td>22.5882</td>\n",
       "      <td>31.8164</td>\n",
       "      <td>19.1520</td>\n",
       "      <td>...</td>\n",
       "      <td>17.785</td>\n",
       "      <td>11.363</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6508.0</td>\n",
       "      <td>12349.0</td>\n",
       "      <td>81.6405</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/1/1959</th>\n",
       "      <td>2483.671</td>\n",
       "      <td>2334.9</td>\n",
       "      <td>17.553</td>\n",
       "      <td>299322.8039</td>\n",
       "      <td>18534.46600</td>\n",
       "      <td>23.2547</td>\n",
       "      <td>24.2022</td>\n",
       "      <td>22.9191</td>\n",
       "      <td>32.3184</td>\n",
       "      <td>19.2746</td>\n",
       "      <td>...</td>\n",
       "      <td>17.796</td>\n",
       "      <td>11.403</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6620.0</td>\n",
       "      <td>12484.0</td>\n",
       "      <td>81.8099</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/1/1959</th>\n",
       "      <td>2498.026</td>\n",
       "      <td>2350.4</td>\n",
       "      <td>17.765</td>\n",
       "      <td>301364.3249</td>\n",
       "      <td>18679.66354</td>\n",
       "      <td>23.6050</td>\n",
       "      <td>24.4032</td>\n",
       "      <td>23.1398</td>\n",
       "      <td>32.4728</td>\n",
       "      <td>19.6670</td>\n",
       "      <td>...</td>\n",
       "      <td>17.777</td>\n",
       "      <td>11.421</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.08</td>\n",
       "      <td>95.3</td>\n",
       "      <td>6753.0</td>\n",
       "      <td>12646.0</td>\n",
       "      <td>80.7315</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6/1/1959</th>\n",
       "      <td>2505.788</td>\n",
       "      <td>2357.4</td>\n",
       "      <td>17.831</td>\n",
       "      <td>301348.7981</td>\n",
       "      <td>18849.75209</td>\n",
       "      <td>23.6319</td>\n",
       "      <td>24.5755</td>\n",
       "      <td>23.3052</td>\n",
       "      <td>32.3184</td>\n",
       "      <td>19.8141</td>\n",
       "      <td>...</td>\n",
       "      <td>17.817</td>\n",
       "      <td>11.463</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6956.0</td>\n",
       "      <td>12926.0</td>\n",
       "      <td>78.6972</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7/1/1959</th>\n",
       "      <td>2504.312</td>\n",
       "      <td>2355.6</td>\n",
       "      <td>17.770</td>\n",
       "      <td>305020.3394</td>\n",
       "      <td>18843.52934</td>\n",
       "      <td>23.0660</td>\n",
       "      <td>24.6042</td>\n",
       "      <td>23.4983</td>\n",
       "      <td>32.7431</td>\n",
       "      <td>20.2064</td>\n",
       "      <td>...</td>\n",
       "      <td>17.835</td>\n",
       "      <td>11.504</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7132.0</td>\n",
       "      <td>13199.0</td>\n",
       "      <td>78.9984</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8/1/1959</th>\n",
       "      <td>2490.236</td>\n",
       "      <td>2342.1</td>\n",
       "      <td>17.877</td>\n",
       "      <td>289435.4569</td>\n",
       "      <td>18963.83587</td>\n",
       "      <td>22.2846</td>\n",
       "      <td>24.4319</td>\n",
       "      <td>23.4431</td>\n",
       "      <td>32.7817</td>\n",
       "      <td>19.6424</td>\n",
       "      <td>...</td>\n",
       "      <td>17.869</td>\n",
       "      <td>11.538</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7309.0</td>\n",
       "      <td>13471.0</td>\n",
       "      <td>77.3045</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9/1/1959</th>\n",
       "      <td>2492.033</td>\n",
       "      <td>2342.1</td>\n",
       "      <td>18.057</td>\n",
       "      <td>293697.8125</td>\n",
       "      <td>18715.96293</td>\n",
       "      <td>22.2577</td>\n",
       "      <td>24.3171</td>\n",
       "      <td>23.3604</td>\n",
       "      <td>32.6659</td>\n",
       "      <td>19.0539</td>\n",
       "      <td>...</td>\n",
       "      <td>17.916</td>\n",
       "      <td>11.572</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7393.0</td>\n",
       "      <td>13657.0</td>\n",
       "      <td>76.8926</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/1/1959</th>\n",
       "      <td>2495.225</td>\n",
       "      <td>2344.8</td>\n",
       "      <td>17.934</td>\n",
       "      <td>294167.8741</td>\n",
       "      <td>18852.86346</td>\n",
       "      <td>22.0960</td>\n",
       "      <td>24.2597</td>\n",
       "      <td>23.2501</td>\n",
       "      <td>32.5115</td>\n",
       "      <td>19.4953</td>\n",
       "      <td>...</td>\n",
       "      <td>17.959</td>\n",
       "      <td>11.611</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7466.0</td>\n",
       "      <td>13804.0</td>\n",
       "      <td>76.2235</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                RPI  W875RX1  DPCERA3M086SBEA    CMRMTSPLx      RETAILx  \\\n",
       "sasdate                                                                   \n",
       "1/1/1959   2442.158   2293.2           17.272  292266.4261  18235.77392   \n",
       "2/1/1959   2451.778   2301.5           17.452  294424.7425  18369.56308   \n",
       "3/1/1959   2467.594   2318.5           17.617  293418.6704  18523.05762   \n",
       "4/1/1959   2483.671   2334.9           17.553  299322.8039  18534.46600   \n",
       "5/1/1959   2498.026   2350.4           17.765  301364.3249  18679.66354   \n",
       "6/1/1959   2505.788   2357.4           17.831  301348.7981  18849.75209   \n",
       "7/1/1959   2504.312   2355.6           17.770  305020.3394  18843.52934   \n",
       "8/1/1959   2490.236   2342.1           17.877  289435.4569  18963.83587   \n",
       "9/1/1959   2492.033   2342.1           18.057  293697.8125  18715.96293   \n",
       "10/1/1959  2495.225   2344.8           17.934  294167.8741  18852.86346   \n",
       "\n",
       "            INDPRO  IPFPNSS  IPFINAL  IPCONGD  IPDCONGD  ...  DNDGRG3M086SBEA  \\\n",
       "sasdate                                                  ...                    \n",
       "1/1/1959   22.0151  23.3984  22.2848  31.5847   18.6861  ...           17.791   \n",
       "2/1/1959   22.4463  23.7142  22.4778  31.8164   18.7842  ...           17.798   \n",
       "3/1/1959   22.7696  23.8577  22.5882  31.8164   19.1520  ...           17.785   \n",
       "4/1/1959   23.2547  24.2022  22.9191  32.3184   19.2746  ...           17.796   \n",
       "5/1/1959   23.6050  24.4032  23.1398  32.4728   19.6670  ...           17.777   \n",
       "6/1/1959   23.6319  24.5755  23.3052  32.3184   19.8141  ...           17.817   \n",
       "7/1/1959   23.0660  24.6042  23.4983  32.7431   20.2064  ...           17.835   \n",
       "8/1/1959   22.2846  24.4319  23.4431  32.7817   19.6424  ...           17.869   \n",
       "9/1/1959   22.2577  24.3171  23.3604  32.6659   19.0539  ...           17.916   \n",
       "10/1/1959  22.0960  24.2597  23.2501  32.5115   19.4953  ...           17.959   \n",
       "\n",
       "           DSERRG3M086SBEA  CES0600000008  CES2000000008  CES3000000008  \\\n",
       "sasdate                                                                   \n",
       "1/1/1959            11.326           2.13           2.45           2.04   \n",
       "2/1/1959            11.343           2.14           2.46           2.05   \n",
       "3/1/1959            11.363           2.15           2.45           2.07   \n",
       "4/1/1959            11.403           2.16           2.47           2.08   \n",
       "5/1/1959            11.421           2.17           2.48           2.08   \n",
       "6/1/1959            11.463           2.17           2.50           2.09   \n",
       "7/1/1959            11.504           2.17           2.51           2.09   \n",
       "8/1/1959            11.538           2.17           2.53           2.07   \n",
       "9/1/1959            11.572           2.17           2.54           2.08   \n",
       "10/1/1959           11.611           2.17           2.56           2.07   \n",
       "\n",
       "           UMCSENTx  DTCOLNVHFNM  DTCTHFNM   INVEST  VIXCLSx  \n",
       "sasdate                                                       \n",
       "1/1/1959        NaN       6476.0   12298.0  84.2043      NaN  \n",
       "2/1/1959        NaN       6476.0   12298.0  83.5280      NaN  \n",
       "3/1/1959        NaN       6508.0   12349.0  81.6405      NaN  \n",
       "4/1/1959        NaN       6620.0   12484.0  81.8099      NaN  \n",
       "5/1/1959       95.3       6753.0   12646.0  80.7315      NaN  \n",
       "6/1/1959        NaN       6956.0   12926.0  78.6972      NaN  \n",
       "7/1/1959        NaN       7132.0   13199.0  78.9984      NaN  \n",
       "8/1/1959        NaN       7309.0   13471.0  77.3045      NaN  \n",
       "9/1/1959        NaN       7393.0   13657.0  76.8926      NaN  \n",
       "10/1/1959       NaN       7466.0   13804.0  76.2235      NaN  \n",
       "\n",
       "[10 rows x 127 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fred_month.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "\n",
    "for m in range(1,13):\n",
    "    for y in range(2000, 2015):\n",
    "        dates.append('{}/1/{}'.format(m, y))\n",
    "        \n",
    "fred = fred_month[fred_month.index.isin(dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = np.log(1 + np.abs(np.nan_to_num(fred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 127)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(fred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.05395774\n",
      "Iteration 2, loss = 0.01730936\n",
      "Iteration 3, loss = 0.01382858\n",
      "Iteration 4, loss = 0.01191178\n",
      "Iteration 5, loss = 0.01084986\n",
      "Iteration 6, loss = 0.01022447\n",
      "Iteration 7, loss = 0.00950844\n",
      "Iteration 8, loss = 0.00901295\n",
      "Iteration 9, loss = 0.00859548\n",
      "Iteration 10, loss = 0.00825059\n",
      "Iteration 11, loss = 0.00797478\n",
      "Iteration 12, loss = 0.00773990\n",
      "Iteration 13, loss = 0.00757077\n",
      "Iteration 14, loss = 0.00738742\n",
      "Iteration 15, loss = 0.00732189\n",
      "Iteration 16, loss = 0.00716984\n",
      "Iteration 17, loss = 0.00705991\n",
      "Iteration 18, loss = 0.00700862\n",
      "Iteration 19, loss = 0.00690797\n",
      "Iteration 20, loss = 0.00676420\n",
      "Iteration 21, loss = 0.00669397\n",
      "Iteration 22, loss = 0.00662079\n",
      "Iteration 23, loss = 0.00652080\n",
      "Iteration 24, loss = 0.00647817\n",
      "Iteration 25, loss = 0.00635599\n",
      "Iteration 26, loss = 0.00632279\n",
      "Iteration 27, loss = 0.00623890\n",
      "Iteration 28, loss = 0.00621708\n",
      "Iteration 29, loss = 0.00617561\n",
      "Iteration 30, loss = 0.00615749\n",
      "Iteration 31, loss = 0.00610638\n",
      "Iteration 32, loss = 0.00610532\n",
      "Iteration 33, loss = 0.00604146\n",
      "Iteration 34, loss = 0.00599985\n",
      "Iteration 35, loss = 0.00604210\n",
      "Iteration 36, loss = 0.00594634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "knn:  0.46898579197652746 0.1731380940870821\n",
      "zero:  0.43177839979789945 0.21804292719958535\n",
      "z-test scores:  {'p_value': 0.02730505214761568, 'mu': 0.037207392178628, 'sigma': 0.10527707964861466, 'win_rate': 0.6379310344827587}\n",
      "z-test loss:  {'p_value': 0.04789185892471615, 'mu': -0.04490483311250325, 'sigma': 0.1417514043005336, 'win_rate': 0.3620689655172414}\n"
     ]
    }
   ],
   "source": [
    "index = SearchSequenceIndex(window=12 * 3, period=12, offset=15)\n",
    "index.build(fred)\n",
    "\n",
    "index.fit(AutoencKnnWrapper(reg, KNeighborsRegressor(n_neighbors=50)))\n",
    "knn_res = index.measure_on_all_train_data(X, offset=15)\n",
    "_, knn_losses = wmsfe(knn_res)\n",
    "_, knn_scores = total(knn_losses)\n",
    "print('knn: ', np.mean(knn_scores), np.mean(knn_losses))\n",
    "\n",
    "index.fit(ZeroModel())\n",
    "zero_res = index.measure_on_all_train_data(X, offset=15)\n",
    "_, zero_losses = wmsfe(zero_res)\n",
    "_, zero_scores = total(zero_losses)\n",
    "print('zero: ', np.mean(zero_scores), np.mean(zero_losses))\n",
    "print('z-test scores: ', z_test(knn_scores, zero_scores))\n",
    "print('z-test loss: ', z_test(knn_losses, zero_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.36426745\n",
      "Iteration 2, loss = 0.15979387\n",
      "Iteration 3, loss = 0.10590827\n",
      "Iteration 4, loss = 0.07116446\n",
      "Iteration 5, loss = 0.04808423\n",
      "Iteration 6, loss = 0.03111591\n",
      "Iteration 7, loss = 0.01981753\n",
      "Iteration 8, loss = 0.01344387\n",
      "Iteration 9, loss = 0.00991182\n",
      "Iteration 10, loss = 0.00780920\n",
      "Iteration 11, loss = 0.00611911\n",
      "Iteration 12, loss = 0.00498006\n",
      "Iteration 13, loss = 0.00409930\n",
      "Iteration 14, loss = 0.00346765\n",
      "Iteration 15, loss = 0.00297382\n",
      "Iteration 16, loss = 0.00260309\n",
      "Iteration 17, loss = 0.00235607\n",
      "Iteration 18, loss = 0.00220216\n",
      "Iteration 19, loss = 0.00206689\n",
      "Iteration 20, loss = 0.00194312\n",
      "Iteration 21, loss = 0.00200111\n",
      "Iteration 22, loss = 0.00182609\n",
      "Iteration 23, loss = 0.00177098\n",
      "Iteration 24, loss = 0.00178243\n",
      "Iteration 25, loss = 0.00170805\n",
      "Iteration 26, loss = 0.00170190\n",
      "Iteration 27, loss = 0.00167714\n",
      "Iteration 28, loss = 0.00167699\n",
      "Iteration 29, loss = 0.00161168\n",
      "Iteration 30, loss = 0.00160634\n",
      "Iteration 31, loss = 0.00157128\n",
      "Iteration 32, loss = 0.00157164\n",
      "Iteration 33, loss = 0.00155958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "knn:  0.2008032846020437 39.16170856963416\n",
      "zero:  0.33289125182923773 1.6253887423863331\n",
      "z-test scores:  {'p_value': 1.0727860312255923e-05, 'mu': -0.132087967227194, 'sigma': 0.18739142799111985, 'win_rate': 0.13043478260869565}\n",
      "z-test loss:  {'p_value': 0.0008204790626533443, 'mu': 37.53631982724784, 'sigma': 70.06240653996335, 'win_rate': 0.9710144927536232}\n"
     ]
    }
   ],
   "source": [
    "index = SearchSequenceIndex(window=12 * 3, period=12, offset=15)\n",
    "index.build(fred_ru)\n",
    "\n",
    "index.fit(AutoencKnnWrapper(reg, KNeighborsRegressor(n_neighbors=50)))\n",
    "knn_res = index.measure_on_all_train_data(train_data.values.T, offset=15)\n",
    "_, knn_losses = wmsfe(knn_res)\n",
    "_, knn_scores = total(knn_losses)\n",
    "print('knn: ', np.mean(knn_scores), np.mean(knn_losses))\n",
    "\n",
    "index.fit(ZeroModel())\n",
    "zero_res = index.measure_on_all_train_data(train_data.values.T, offset=15)\n",
    "_, zero_losses = wmsfe(zero_res)\n",
    "_, zero_scores = total(zero_losses)\n",
    "print('zero: ', np.mean(zero_scores), np.mean(zero_losses))\n",
    "\n",
    "print('z-test scores: ', z_test(knn_scores, zero_scores))\n",
    "print('z-test loss: ', z_test(knn_losses, zero_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowm = 12 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
